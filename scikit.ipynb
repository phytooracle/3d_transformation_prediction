{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 07:56:58.583527: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-02 07:56:58.679841: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-02 07:56:59.888992: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, StackingRegressor, VotingRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_data(group):\n",
    "    data_list = []\n",
    "    dt_list = []\n",
    "    filename_list = []\n",
    "\n",
    "    for key in group.keys():\n",
    "        \n",
    "        for key2 in group[key].keys():\n",
    "\n",
    "            dt = datetime.strptime(' '.join([key.split('__')[-2], key.split('__')[-1].replace('-', ':')]), \n",
    "                                        \"%Y-%m-%d %H:%M:%S:%f\")\n",
    "            data = np.array(group[key][key2])\n",
    "            data_list.append(data)\n",
    "            dt_list.append(dt)\n",
    "            filename_list.append(key2)\n",
    "    \n",
    "    df = pd.DataFrame({'filename': filename_list, 'data': data_list, 'dt': dt_list})\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_translations(positive, negative):\n",
    "    x_translation_list = []\n",
    "    y_translation_list = []\n",
    "    dt_list = []\n",
    "\n",
    "    for i in range(len(positive['data'])):\n",
    "        data = positive['data'][i]\n",
    "        x_translation = data[0, 3]\n",
    "        y_translation = data[2, 3]\n",
    "        dt = positive['dt'][i].strftime('%H:%M:%S')\n",
    "        x_translation_list.append(x_translation)\n",
    "        y_translation_list.append(y_translation)\n",
    "        dt_list.append(dt)\n",
    "\n",
    "    positive_df = pd.DataFrame({'x_translation': x_translation_list, 'y_translation': y_translation_list, 'dt': dt_list})\n",
    "\n",
    "    x_translation_list = []\n",
    "    y_translation_list = []\n",
    "    dt_list = []\n",
    "\n",
    "    for i in range(len(negative['data'])):\n",
    "        data = negative['data'][i]\n",
    "        x_translation = data[0, 3]\n",
    "        y_translation = data[2, 3]\n",
    "        dt = negative['dt'][i].strftime('%H:%M:%S')\n",
    "        x_translation_list.append(x_translation)\n",
    "        y_translation_list.append(y_translation)\n",
    "        dt_list.append(dt)\n",
    "\n",
    "    negative_df = pd.DataFrame({'x_translation': x_translation_list, 'y_translation': y_translation_list, 'dt': dt_list})\n",
    "\n",
    "    return positive_df, negative_df\n",
    "\n",
    "\n",
    "def plot_translations(positive_df, negative_df):\n",
    "    positive_df['type'] = 'Positive'\n",
    "    negative_df['type'] = 'Negative'\n",
    "\n",
    "    df = pd.concat([positive_df, negative_df])\n",
    "\n",
    "    df = df.melt(id_vars=['dt', 'type'], value_vars=['x_translation', 'y_translation'], var_name='axis', value_name='translation')\n",
    "\n",
    "    g = sns.relplot(data=df, x='dt', y='translation', hue='axis', col='type', kind='line')\n",
    "\n",
    "    # Reduce the number of x-axis labels\n",
    "    x_ticks = g.axes[0][0].get_xticks()\n",
    "    g.set(xticks=x_ticks[::10])\n",
    "    g.set_xticklabels(df['dt'].unique()[::10], rotation=45)\n",
    "\n",
    "    # Update axis titles\n",
    "    g.set_axis_labels('Time', 'Translation')    \n",
    "    g.set_titles(row_template = '{row_name}', col_template = '{col_name}')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def normalize_data(df):\n",
    "    # Create a StandardScaler object\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler to the data\n",
    "    scaler.fit(df)\n",
    "    \n",
    "    # Transform the data\n",
    "    df_norm = scaler.transform(df)\n",
    "    \n",
    "    return df_norm, scaler\n",
    "\n",
    "\n",
    "def denormalize_y(y_norm, scaler):\n",
    "    # Transform the data back to its original scale\n",
    "    y = scaler.inverse_transform(y_norm)\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "def sign_mse_loss(y_true, y_pred):\n",
    "    # Calculate the mean squared error\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    # Calculate the sign error\n",
    "    sign_error = np.mean((np.sign(y_true) != np.sign(y_pred)).astype(float))\n",
    "    \n",
    "    # Calculate the total loss\n",
    "    loss = mse + sign_error\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_metrics(y_test, y_pred, X):\n",
    "    \n",
    "    # Calculate the evaluation metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    adj_r2 = 1 - (1 - r2) * (y_test.shape[0] - 1) / (y_test.shape[0] - X.shape[1] - 1)\n",
    "    loss = sign_mse_loss(y_test, y_pred)\n",
    "\n",
    "    print(f'Mean Absolute Error: {mae:.2f}')\n",
    "    print(f'Root Mean Squared Error: {rmse:.2f}')\n",
    "    print(f'R-squared: {r2:.2f}')\n",
    "    print(f'Adjusted R-squared: {adj_r2:.2f}')\n",
    "    print(f'Sign MSE Loss: {loss:.2f}')\n",
    "\n",
    "    return mae, rmse, r2, adj_r2, loss\n",
    "\n",
    "\n",
    "# def get_metrics(y_test, y_pred, X):\n",
    "    \n",
    "#     # Calculate the evaluation metrics\n",
    "#     mae = np.abs(y_test - y_pred)\n",
    "#     rmse = np.sqrt((y_test - y_pred) ** 2)\n",
    "#     r2 = r2_score(y_test, y_pred, multioutput='raw_values')\n",
    "#     adj_r2 = np.full_like(r2, np.nan)\n",
    "#     loss = sign_mse_loss(y_test, y_pred)\n",
    "\n",
    "#     # print(f'Mean Absolute Error: {mae}')\n",
    "#     # print(f'Root Mean Squared Error: {rmse}')\n",
    "#     print(f'R-squared: {r2}')\n",
    "#     # print(f'Adjusted R-squared: {adj_r2}')\n",
    "#     # print(f'Sign MSE Loss: {loss}')\n",
    "\n",
    "#     return mae, rmse, r2, adj_r2, loss\n",
    "\n",
    "\n",
    "\n",
    "def plot_results(results):\n",
    "    \n",
    "    # Extract the data from the results\n",
    "    names = [x[0] for x in results]\n",
    "    mae_values = [x[1] for x in results]\n",
    "    rmse_values = [x[2] for x in results]\n",
    "    r2_values = [x[3] for x in results]\n",
    "\n",
    "    # Create a bar chart of the MAE values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(names, mae_values)\n",
    "    plt.title('Mean Absolute Error')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Create a bar chart of the RMSE values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(names, rmse_values)\n",
    "    plt.title('Root Mean Squared Error')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Create a bar chart of the R-squared values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(names, r2_values)\n",
    "    plt.title('R-squared')\n",
    "    plt.ylabel('R-squared')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all H5 files\n",
    "# h5_files = glob.glob('scanner3DTop_Transformations/TESTDATASET_*/*.h5')\n",
    "h5_files = glob.glob('scanner3DTop_Transformations/*/*.h5')\n",
    "df_list = []\n",
    "\n",
    "for h5_file in h5_files:\n",
    "\n",
    "    # Open H5 file\n",
    "    h5 = h5py.File(h5_file, 'r')\n",
    "\n",
    "    # Get negative direction transformations\n",
    "    negative = get_group_data(group=h5[\"EW/individual/transformations/negative/\"])\n",
    "\n",
    "    # Get positive direction transformations\n",
    "    positive = get_group_data(group=h5[\"EW/individual/transformations/positive/\"])\n",
    "\n",
    "    # Combine positive and negative transformations\n",
    "    transformations = pd.concat([positive, negative])\n",
    "\n",
    "    # Extract Environment Logger data\n",
    "    df = pd.read_hdf(h5_file, 'environment_logger') \n",
    "\n",
    "    # Merge transformations and Environment logger data\n",
    "    df = df.merge(transformations, on='filename')\n",
    "\n",
    "    # Drop unwanted columns\n",
    "    df = df.drop(['directories', 'filename', 'dt'], axis=1)\n",
    "\n",
    "    # Flatten the transformations\n",
    "    df['data'] = df['data'].apply(lambda x: x.flatten())\n",
    "\n",
    "    # dummies_field = pd.get_dummies(df['field'], prefix='field')\n",
    "    # dummies_scan_direction = pd.get_dummies(df['scan_direction'], prefix='scan_direction')\n",
    "    # df = pd.concat([df, dummies_field, dummies_scan_direction], axis=1)\n",
    "    # df = df.drop(['field', 'scan_direction'], axis=1)\n",
    "    df['field'] = df['field'].map({'north': 0.0, 'south': 1.0})\n",
    "    df['scan_direction'] = df['scan_direction'].map({'Negative': 0.0, 'Positive': 1.0})\n",
    "\n",
    "    # Reset the index of your DataFrame\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Add to list\n",
    "    df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "df = df.dropna()\n",
    "# df = df[df['scan_direction']==1]\n",
    "df = df.drop(['brightness'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your data into input features X and output targets y\n",
    "X = df.drop('data', axis=1)\n",
    "\n",
    "# Extract X, Y, Z transformations\n",
    "df['data'] = df['data'].apply(lambda x: x.reshape(4, 4)[:3,3])#[:3])\n",
    "\n",
    "# Extract the transformations from the \"data\" column of the dataframe\n",
    "y = np.stack(df['data'].values)\n",
    "\n",
    "# Flatten the transformations to create a 2D array\n",
    "y = y.reshape(y.shape[0], -1)\n",
    "\n",
    "# Convert any Timestamps in the input features to a numerical representation\n",
    "X = X.apply(lambda x: x.astype(int) if np.issubdtype(x.dtype, np.datetime64) else x)\n",
    "\n",
    "# Convert only the columns with numeric data types to float\n",
    "X[X.select_dtypes(include='number').columns] = X.select_dtypes(include='number').astype(float)\n",
    "\n",
    "# Normalize the y variable\n",
    "# X, scaler = normalize_data(X)\n",
    "\n",
    "# Split your data into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create a custom scorer\n",
    "sign_mse_scorer = make_scorer(sign_mse_loss, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5.,   35.,    0.],\n",
       "       [ -20.,  -40.,    0.],\n",
       "       [-135.,  -95.,    0.],\n",
       "       [  10.,   30.,    5.],\n",
       "       [-155., -140.,    0.],\n",
       "       [  15.,   15.,    0.],\n",
       "       [ -55.,   -5.,    0.],\n",
       "       [ -35.,   25.,    0.],\n",
       "       [ -45.,    5.,    0.],\n",
       "       [  20.,   40.,    0.],\n",
       "       [   5.,   15.,    0.],\n",
       "       [  20.,   -5.,    0.],\n",
       "       [ 210.,  150.,    0.],\n",
       "       [ 120.,   50.,    0.],\n",
       "       [ -55.,   10.,    0.],\n",
       "       [   0.,    0.,    0.],\n",
       "       [ -35.,  -15.,    0.],\n",
       "       [ -15.,    0.,    0.],\n",
       "       [ -35.,    5.,    0.],\n",
       "       [  20.,  -20.,    0.],\n",
       "       [ -25.,    5.,    0.],\n",
       "       [ -30.,    5.,    0.],\n",
       "       [ -15.,    5.,    0.],\n",
       "       [ 150.,   70.,    0.],\n",
       "       [ 105.,   15.,    0.],\n",
       "       [  60.,   45.,    0.],\n",
       "       [  20.,   35.,    0.],\n",
       "       [ -55.,  -35.,    0.],\n",
       "       [ -35.,  -45.,    0.],\n",
       "       [ -25.,  -10.,    0.],\n",
       "       [ 135.,   60.,    0.],\n",
       "       [ -85.,  -60.,    0.],\n",
       "       [ -30.,    0.,    0.],\n",
       "       [ 150.,   75.,    0.],\n",
       "       [  60.,   85.,    0.],\n",
       "       [-105.,  -95.,    0.],\n",
       "       [ -45.,   -5.,    0.],\n",
       "       [  45.,   80.,   10.],\n",
       "       [ -70.,  -80.,    0.],\n",
       "       [  45.,   55.,    0.],\n",
       "       [ 130.,   65.,    0.],\n",
       "       [ -40.,  -15.,    0.],\n",
       "       [ 220.,  140.,    0.],\n",
       "       [   0.,  -25.,    0.],\n",
       "       [ -15.,  -30.,    0.],\n",
       "       [  15.,   40.,    0.],\n",
       "       [ -60.,  -35.,    0.],\n",
       "       [-245., -190.,    0.],\n",
       "       [  45.,   -5.,    0.],\n",
       "       [  -5.,   -5.,   10.],\n",
       "       [  15.,   40.,    0.],\n",
       "       [ 205.,  145.,    0.],\n",
       "       [   0.,   45.,    0.],\n",
       "       [ -10.,   10.,    0.],\n",
       "       [   5.,  -10.,    0.],\n",
       "       [ -35.,  -60.,    0.],\n",
       "       [ -35.,    0.,    0.],\n",
       "       [  25.,   45.,    0.],\n",
       "       [  20.,   20.,    0.],\n",
       "       [  70.,   75.,    0.],\n",
       "       [  -5.,   20.,    0.],\n",
       "       [  60.,   10.,    0.],\n",
       "       [ -10.,   15.,    0.],\n",
       "       [ 150.,   55.,    0.],\n",
       "       [  55.,   75.,    0.],\n",
       "       [  30.,  -45.,    0.],\n",
       "       [   5.,   35.,    0.],\n",
       "       [  45.,   10.,    0.],\n",
       "       [ -20.,    5.,    0.],\n",
       "       [  10.,   25.,    0.],\n",
       "       [ -20.,    5.,    0.],\n",
       "       [ 250.,  295.,    0.],\n",
       "       [ -15.,   -5.,    0.],\n",
       "       [  75.,  135.,    0.],\n",
       "       [  -5.,   10.,    0.],\n",
       "       [ -45.,  -30.,    0.],\n",
       "       [  90.,  105.,    0.],\n",
       "       [  20.,   65.,    0.],\n",
       "       [  25.,   70.,    0.],\n",
       "       [ -60.,    0.,   10.],\n",
       "       [  70.,   85.,    0.],\n",
       "       [  20.,   40.,    0.],\n",
       "       [  10.,   35.,    0.],\n",
       "       [  15.,   20.,    0.],\n",
       "       [ -20.,  -15.,    0.],\n",
       "       [ 170.,  200.,    0.],\n",
       "       [  -5.,   -5.,    0.],\n",
       "       [ -45.,    0.,    0.],\n",
       "       [ 150.,   55.,    0.],\n",
       "       [ -10.,   15.,    0.],\n",
       "       [  35.,   35.,    0.],\n",
       "       [ -45.,  -15.,    0.],\n",
       "       [ -15.,   -5.,    0.],\n",
       "       [ -25.,    0.,    0.],\n",
       "       [  20.,   35.,    0.],\n",
       "       [ 125.,  160.,    0.],\n",
       "       [ -25.,  -15.,    0.],\n",
       "       [-320., -300.,    0.],\n",
       "       [  95.,   35.,    0.],\n",
       "       [ 150.,  135.,    0.],\n",
       "       [  20.,   30.,    0.],\n",
       "       [  65.,  105.,    0.],\n",
       "       [  -5.,   40.,    0.],\n",
       "       [   0.,   30.,    0.],\n",
       "       [ -30.,    5.,    0.],\n",
       "       [ -10.,   40.,    0.],\n",
       "       [ -50.,  -10.,    0.],\n",
       "       [  50.,   30.,    0.],\n",
       "       [ 105.,  175.,    0.],\n",
       "       [ -80.,  -70.,    0.],\n",
       "       [  20.,   65.,    0.],\n",
       "       [  45.,  120.,    0.],\n",
       "       [   5.,   30.,    0.],\n",
       "       [ 145.,   70.,    0.],\n",
       "       [ -10.,   20.,    0.],\n",
       "       [-100.,  -90.,    0.],\n",
       "       [   5.,   30.,    0.],\n",
       "       [ -20.,   -5.,    0.],\n",
       "       [  65.,   95.,    0.],\n",
       "       [ -55.,  -50.,    0.],\n",
       "       [ -95.,  -30.,    0.],\n",
       "       [  45.,   70.,    0.],\n",
       "       [ -40.,   15.,    0.],\n",
       "       [  75.,  -20.,    0.],\n",
       "       [ 140.,  105.,    0.],\n",
       "       [ -35.,   15.,    0.],\n",
       "       [  55.,   55.,    0.],\n",
       "       [   0.,   25.,    0.],\n",
       "       [   0.,  -25.,    0.],\n",
       "       [ 220.,  110.,    0.],\n",
       "       [-125., -125.,    0.],\n",
       "       [ -35.,  -20.,    0.],\n",
       "       [ 110.,  140.,    0.],\n",
       "       [  -5.,  -10.,    0.],\n",
       "       [  80.,   85.,    0.],\n",
       "       [  55.,  100.,    5.],\n",
       "       [  15.,   15.,    0.],\n",
       "       [  20.,   45.,    0.],\n",
       "       [  -5.,   40.,   10.],\n",
       "       [ -20.,   10.,    0.],\n",
       "       [ -40.,  -10.,    0.],\n",
       "       [ -10.,   55.,    0.],\n",
       "       [  -5.,    0.,    0.],\n",
       "       [ -45.,  -35.,    0.],\n",
       "       [  60.,  125.,    0.],\n",
       "       [ 105.,   25.,    0.],\n",
       "       [  -5.,    0.,    0.],\n",
       "       [ -80.,  -80.,    0.],\n",
       "       [ -35.,   15.,    0.],\n",
       "       [  65.,   85.,    0.],\n",
       "       [ -25.,  -50.,    0.],\n",
       "       [ -45.,   10.,    0.],\n",
       "       [  45.,   70.,    0.],\n",
       "       [  60.,   40.,    0.],\n",
       "       [-105.,  -85.,    0.],\n",
       "       [  95.,   70.,    0.],\n",
       "       [  15.,   40.,    0.],\n",
       "       [  60.,   75.,    0.],\n",
       "       [ -25.,   20.,    0.],\n",
       "       [ 145.,  130.,    0.],\n",
       "       [  75.,   90.,    0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\n",
      "Mean Absolute Error: 32.97\n",
      "Root Mean Squared Error: 56.14\n",
      "R-squared: 0.01\n",
      "Adjusted R-squared: -0.08\n",
      "Sign MSE Loss: 3151.96\n",
      "\n",
      "\n",
      "Ridge Regression\n",
      "Mean Absolute Error: 32.89\n",
      "Root Mean Squared Error: 56.20\n",
      "R-squared: 0.10\n",
      "Adjusted R-squared: 0.01\n",
      "Sign MSE Loss: 3159.16\n",
      "\n",
      "\n",
      "Lasso Regression\n",
      "Mean Absolute Error: 32.90\n",
      "Root Mean Squared Error: 56.28\n",
      "R-squared: 0.04\n",
      "Adjusted R-squared: -0.05\n",
      "Sign MSE Loss: 3168.42\n",
      "\n",
      "\n",
      "Elastic Net Regression\n",
      "Mean Absolute Error: 32.90\n",
      "Root Mean Squared Error: 56.28\n",
      "R-squared: 0.04\n",
      "Adjusted R-squared: -0.05\n",
      "Sign MSE Loss: 3167.65\n",
      "\n",
      "\n",
      "Random Forest Regressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eg/miniconda3/envs/tf/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=8.41548e-36): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 31.00\n",
      "Root Mean Squared Error: 60.74\n",
      "R-squared: 0.20\n",
      "Adjusted R-squared: 0.12\n",
      "Sign MSE Loss: 3690.19\n",
      "\n",
      "\n",
      "BaggingRegressor\n",
      "Mean Absolute Error: 33.50\n",
      "Root Mean Squared Error: 65.00\n",
      "R-squared: 0.06\n",
      "Adjusted R-squared: -0.03\n",
      "Sign MSE Loss: 4225.45\n",
      "\n",
      "\n",
      "ExtraTreesRegressor\n",
      "Mean Absolute Error: 30.68\n",
      "Root Mean Squared Error: 58.45\n",
      "R-squared: 0.24\n",
      "Adjusted R-squared: 0.16\n",
      "Sign MSE Loss: 3416.91\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a list of models to evaluate\n",
    "models = [\n",
    "    ('Linear Regression', LinearRegression()),\n",
    "    ('Ridge Regression', Ridge(random_state=0)),\n",
    "    ('Lasso Regression', Lasso(random_state=0)),\n",
    "    ('Elastic Net Regression', ElasticNet(random_state=0)),\n",
    "    ('Random Forest Regressor', RandomForestRegressor(random_state=0)),\n",
    "    ('BaggingRegressor', BaggingRegressor(random_state=0)),\n",
    "    ('ExtraTreesRegressor', ExtraTreesRegressor(random_state=0))\n",
    "    # ('Support Vector Regression', SVR()),\n",
    "    # ('Neural Network', MLPRegressor(random_state=0))\n",
    "]\n",
    "\n",
    "# Evaluate each model\n",
    "results = []\n",
    "for name, model in models:\n",
    "    print(name)\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the evaluation metrics\n",
    "    mae, rmse, r2, adj_r2, loss = get_metrics(y_test, y_pred, X_test)\n",
    "    print('\\n')\n",
    "\n",
    "    # Save the results\n",
    "    results.append((name, mae, rmse, r2, adj_r2, loss))\n",
    "\n",
    "# del y_pred\n",
    "# plot_results(results=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5., 35.,  0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.8 , 35.85,  0.  ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.stack(df['data'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-30.,  10.,   0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(y.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-30.,  10.,   0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'n_estimators': [10, 50, 100, 200], \n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'criterion': ['squared_error', 'absolute_error', 'friedman_mse'], #, 'poisson'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 8, 10],\n",
    "    'min_weight_fraction_leaf': [0.0, 0.1, 0.2],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'max_leaf_nodes': [None, 10, 20, 30],\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2],\n",
    "    'bootstrap': [True, False],\n",
    "    'oob_score': [True, False],\n",
    "    'warm_start': [True, False],\n",
    "    'ccp_alpha': [0.0, 0.1, 0.2],\n",
    "    'max_samples': [None, 0.5, 0.8]\n",
    "}\n",
    "\n",
    "# Create a random forest regressor\n",
    "model = RandomForestRegressor(random_state=0, n_jobs=-1)\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_absolute_error') #neg_mean_squared_error\n",
    "\n",
    "# Fit the grid search object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train a random forest regressor with the best parameters\n",
    "best_model = RandomForestRegressor(**best_params, random_state=0, n_jobs=-1)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mae, rmse, r2, adj_r2, loss = get_metrics(y_test, y_pred, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'n_estimators': [10, 50, 100, 200, 300, 400, 500],\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "# }\n",
    "\n",
    "# # Create an ExtraTreesRegressor\n",
    "# model = ExtraTreesRegressor(random_state=0, n_jobs=-1)\n",
    "# # model = BaggingRegressor(model, random_state=0, n_jobs=-1)\n",
    "\n",
    "# # Create a grid search object\n",
    "# grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# # Fit the grid search object to the data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best parameters\n",
    "# best_params = grid_search.best_params_\n",
    "\n",
    "# # Train an ExtraTreesRegressor with the best parameters\n",
    "# best_model = ExtraTreesRegressor(**best_params, random_state=0, n_jobs=-1)\n",
    "# # best_model = BaggingRegressor(best_model, random_state=0, n_jobs=-1)\n",
    "# best_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = best_model.predict(X_test)\n",
    "\n",
    "# mae, rmse, r2, adj_r2, loss = get_metrics(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denormalize_y(y_test, scaler)\n",
    "\n",
    "# denormalize_y(y_pred, scaler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
