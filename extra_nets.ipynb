{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=7.88339291923655e-07\n",
    "l2=2.0285042672959013e-10\n",
    "model_three = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(train_features.shape[1]), name=\"in_layer\"),\n",
    "        layers.Dense(112, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), name=\"dl1\"),\n",
    "        layers.Dense(112, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), name=\"dl2\"),\n",
    "        layers.Dense(112, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), name=\"dl3\"),\n",
    "        layers.Dense(112, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), name=\"dl4\"),\n",
    "        layers.Dense(3, name=\"out_layer\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "lr = 0.031502784477953634\n",
    "b1 =  0.8043343313655912\n",
    "b2 = 0.9349196970511437\n",
    "cv = 0.9312479622149389\n",
    "num_batches=64\n",
    "# opt='sgd'\n",
    "scaler='standard_scaler'\n",
    "loss_func = 'huber_loss'\n",
    "num_epochs=739\n",
    "\n",
    "model_three.summary()\n",
    "# opt = Adam(learning_rate=lr, beta_1=b1, beta_2=b2, clipvalue=cv)\n",
    "opt = SGD(learning_rate=lr) #, clipvalue=cv)\n",
    "model_three.compile(loss=loss_func, optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=30)\n",
    "history_three = model_three.fit(scaled_train_features, train_labels, validation_split=0.2, verbose=1, epochs=num_epochs, batch_size=num_batches) #, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the final performance of the model on the validation data\n",
    "val_loss = history_three.history['val_loss'][-1]\n",
    "print(\"Val loss: \", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=7.88339291923655e-07\n",
    "l2=2.0285042672959013e-10\n",
    "model_2 = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(5,2), name=\"in_layer\"),\n",
    "        layers.Dense(112, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), name=\"dl1\"),\n",
    "        #layers.Dense(112, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), name=\"dl2\"),\n",
    "        #layers.Dense(112, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), name=\"dl3\"),\n",
    "        #layers.Dense(112, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), name=\"dl4\"),\n",
    "        #layers.Dense(112, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), name=\"dl5\"),\n",
    "        layers.Dense(3, name=\"out_layer\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "lr = 0.031502784477953634\n",
    "b1 =  0.8043343313655912\n",
    "b2 = 0.9349196970511437\n",
    "cv = 0.9312479622149389\n",
    "num_batches=64\n",
    "# opt='sgd'\n",
    "scaler='standard_scaler'\n",
    "loss_func = 'huber_loss'\n",
    "num_epochs=739\n",
    "\n",
    "model_2.summary()\n",
    "# opt = Adam(learning_rate=lr, beta_1=b1, beta_2=b2, clipvalue=cv)\n",
    "opt = SGD(learning_rate=lr) #, clipvalue=cv)\n",
    "model_2.compile(loss=loss_func, optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=30)\n",
    "history_2 = model_2.fit(test_data_two, train_labels, validation_split=0.2, verbose=1, epochs=num_epochs, batch_size=num_batches) #, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the final performance of the model on the validation data\n",
    "val_loss = history_2.history['val_loss'][-1]\n",
    "print(\"Val loss: \", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=4.5927655210613596e-08 \n",
    "l2=6.769705259483123e-10\n",
    "model_4 = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(train_features.shape[1]), name=\"in_layer\"),\n",
    "        layers.Dense(112, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), name=\"dl1\"),\n",
    "        layers.Dense(112, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), name=\"dl2\"),\n",
    "        layers.Dense(112, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), name=\"dl3\"),\n",
    "        layers.Reshape((1,112), name=\"reshape_layer\"),\n",
    "        layers.SimpleRNN(112, activation='tanh', name=\"rnn2_layer\"),\n",
    "        layers.Dense(3, name=\"out_layer\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "lr = 0.0017172015688603477\n",
    "b1 = 0.9192003556241318\n",
    "b2 = 0.9431954647698297\n",
    "cv = 0.8273933576659364\n",
    "num_batches=32\n",
    "# opt='sgd'\n",
    "scaler='standard_scaler'\n",
    "loss_func = 'huber_loss'\n",
    "num_epochs=998\n",
    "\n",
    "model_4.summary()\n",
    "opt = Adam(learning_rate=lr, beta_1=b1, beta_2=b2, clipvalue=cv)\n",
    "#opt = SGD(learning_rate=lr) #, clipvalue=cv)\n",
    "model_4.compile(loss=loss_func, optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=30)\n",
    "history_4 = model_4.fit(scaled_train_features, train_labels, validation_split=0.2, verbose=1, epochs=num_epochs, batch_size=num_batches) #, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the final performance of the model on the validation data\n",
    "val_loss = history_4.history['val_loss'][-1]\n",
    "accuracy = history_4.history['accuracy'][-1]\n",
    "print(\"Val loss: \", val_loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=7.88339291923655e-07\n",
    "l2=2.0285042672959013e-10\n",
    "model_3 = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(train_features.shape[1]), name=\"in_layer\"),\n",
    "        layers.Dense(112, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), name=\"dl1\"),\n",
    "        layers.Dense(112, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), name=\"dl2\"),\n",
    "        layers.Dense(112, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), name=\"dl3\"),\n",
    "        layers.Dense(112, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), name=\"dl4\"),\n",
    "        layers.Dense(112, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), name=\"dl5\"),\n",
    "        layers.Dense(112, activation='tanh', kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), name=\"dl6\"),\n",
    "        layers.Reshape((1,112), name=\"reshape_layer\"),\n",
    "        #layers.SimpleRNN(112, activation='tanh', name=\"rnn_layer\", return_sequences=True),\n",
    "        layers.SimpleRNN(112, activation='tanh', name=\"rnn2_layer\"),\n",
    "        layers.Dense(3, name=\"out_layer\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "lr = 0.031502784477953634\n",
    "b1 =  0.8043343313655912\n",
    "b2 = 0.9349196970511437\n",
    "cv = 0.9312479622149389\n",
    "num_batches=32\n",
    "# opt='sgd'\n",
    "scaler='standard_scaler'\n",
    "loss_func = 'huber_loss'\n",
    "num_epochs=739\n",
    "\n",
    "model_3.summary()\n",
    "# opt = Adam(learning_rate=lr, beta_1=b1, beta_2=b2, clipvalue=cv)\n",
    "opt = SGD(learning_rate=lr, clipvalue=cv)\n",
    "model_3.compile(loss=loss_func, optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=30)\n",
    "history_3 = model_3.fit(scaled_train_features, train_labels, validation_split=0.2, verbose=1, epochs=num_epochs, batch_size=num_batches) #, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the final performance of the model on the validation data\n",
    "val_loss = history_3.history['val_loss'][-1]\n",
    "print(\"Val loss: \", val_loss)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
