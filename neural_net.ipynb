{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_data(group):\n",
    "    data_list = []\n",
    "    dt_list = []\n",
    "    filename_list = []\n",
    "\n",
    "    for key in group.keys():\n",
    "        \n",
    "        for key2 in group[key].keys():\n",
    "\n",
    "            dt = datetime.strptime(' '.join([key.split('__')[-2], key.split('__')[-1].replace('-', ':')]), \n",
    "                                        \"%Y-%m-%d %H:%M:%S:%f\")\n",
    "            data = np.array(group[key][key2])\n",
    "            data_list.append(data)\n",
    "            dt_list.append(dt)\n",
    "            filename_list.append(key2)\n",
    "    \n",
    "    df = pd.DataFrame({'filename': filename_list, 'data': data_list, 'dt': dt_list})\n",
    "    return df\n",
    "\n",
    "def get_translations(positive, negative):\n",
    "    x_translation_list = []\n",
    "    y_translation_list = []\n",
    "    dt_list = []\n",
    "\n",
    "    for i in range(len(positive['data'])):\n",
    "        data = positive['data'][i]\n",
    "        x_translation = data[0, 3]\n",
    "        y_translation = data[2, 3]\n",
    "        dt = positive['dt'][i].strftime('%H:%M:%S')\n",
    "        x_translation_list.append(x_translation)\n",
    "        y_translation_list.append(y_translation)\n",
    "        dt_list.append(dt)\n",
    "\n",
    "    positive_df = pd.DataFrame({'x_translation': x_translation_list, 'y_translation': y_translation_list, 'dt': dt_list})\n",
    "\n",
    "    x_translation_list = []\n",
    "    y_translation_list = []\n",
    "    dt_list = []\n",
    "\n",
    "    for i in range(len(negative['data'])):\n",
    "        data = negative['data'][i]\n",
    "        x_translation = data[0, 3]\n",
    "        y_translation = data[2, 3]\n",
    "        dt = negative['dt'][i].strftime('%H:%M:%S')\n",
    "        x_translation_list.append(x_translation)\n",
    "        y_translation_list.append(y_translation)\n",
    "        dt_list.append(dt)\n",
    "\n",
    "    negative_df = pd.DataFrame({'x_translation': x_translation_list, 'y_translation': y_translation_list, 'dt': dt_list})\n",
    "\n",
    "    return positive_df, negative_df\n",
    "\n",
    "def plot_translations(positive_df, negative_df):\n",
    "    positive_df['type'] = 'Positive'\n",
    "    negative_df['type'] = 'Negative'\n",
    "\n",
    "    df = pd.concat([positive_df, negative_df])\n",
    "\n",
    "    df = df.melt(id_vars=['dt', 'type'], value_vars=['x_translation', 'y_translation'], var_name='axis', value_name='translation')\n",
    "\n",
    "    g = sns.relplot(data=df, x='dt', y='translation', hue='axis', col='type', kind='line')\n",
    "\n",
    "    # Reduce the number of x-axis labels\n",
    "    x_ticks = g.axes[0][0].get_xticks()\n",
    "    g.set(xticks=x_ticks[::10])\n",
    "    g.set_xticklabels(df['dt'].unique()[::10], rotation=45)\n",
    "\n",
    "    # Update axis titles\n",
    "    g.set_axis_labels('Time', 'Translation')    \n",
    "    g.set_titles(row_template = '{row_name}', col_template = '{col_name}')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss(history, num_epochs):\n",
    "    plt.plot(history.history['loss'][:num_epochs], label='loss')\n",
    "    plt.plot(history.history['val_loss'][:num_epochs], label='val_loss')\n",
    "    # plt.ylim([0, 10])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error [mm]')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "\n",
    "def build_and_compile_model(train_features, train_labels, n_trials=100): #, epochs=500):\n",
    "    def objective(trial):\n",
    "        if trial.number ==0:\n",
    "            learning_rate = 0.0017172015688603477\n",
    "            beta_1 =  0.8539642649814688\n",
    "            beta_2 = 0.9358548594051231\n",
    "            clipvalue = 0.964758137195115\n",
    "            batch_size=32\n",
    "            num_layers = 3\n",
    "            num_neurons = 112\n",
    "            activation = 'tanh'\n",
    "            l1=4.5927655210613596e-08 \n",
    "            l2=6.769705259483123e-10\n",
    "            optimizer_name = 'adam'\n",
    "            scaler_name='standard_scaler'\n",
    "            loss_name = 'huber_loss'\n",
    "            epochs=500\n",
    "        else:   \n",
    "            # Define the search space for the hyperparameters\n",
    "            learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-0, log=True)\n",
    "            # beta_1 = trial.suggest_uniform('beta_1', 0.8, 0.99)\n",
    "            # beta_2 = trial.suggest_uniform('beta_2', 0.9, 0.999)\n",
    "            #clipvalue = trial.suggest_uniform('clipvalue', 0.1, 1.0)\n",
    "            #batch_size = trial.suggest_int('batch_size', 32, 256, 32)\n",
    "            #num_layers = trial.suggest_int('num_layers', 1, 5)\n",
    "            #num_neurons = trial.suggest_int('num_neurons', 16, 128, 16)\n",
    "            #activation = trial.suggest_categorical('activation', ['sigmoid', 'tanh', 'relu']) #'linear', \n",
    "            # l1 = trial.suggest_float('l1', 1e-10, 1e-2, log=True)\n",
    "            # l2 = trial.suggest_float('l2', 1e-10, 1e-2, log=True)\n",
    "            #optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'sgd', 'rmsprop'])\n",
    "            #loss_name = trial.suggest_categorical('loss', ['mean_absolute_error', 'mean_squared_error', 'huber_loss']) #, 'log_cosh'])\n",
    "            #scaler_name = trial.suggest_categorical('scaler', ['normalizer', 'standard_scaler'])\n",
    "            # epochs = trial.suggest_int('epochs', 100, 1000)\n",
    "\n",
    "\n",
    "            learning_rate = 0.0017172015688603477\n",
    "            beta_1 =  0.8539642649814688\n",
    "            beta_2 = 0.9358548594051231\n",
    "            clipvalue =0.964758137195115\n",
    "            batch_size=32\n",
    "            num_layers = 3\n",
    "            num_neurons = 112\n",
    "            activation = 'tanh'\n",
    "            l1=4.5927655210613596e-08 \n",
    "            l2=6.769705259483123e-10\n",
    "            optimizer_name = 'adam'\n",
    "            scaler_name='standard_scaler'\n",
    "            loss_name = 'huber_loss'\n",
    "            epochs=500\n",
    "\n",
    "        # Scale the data\n",
    "        if scaler_name == 'normalizer':\n",
    "            scaler = MinMaxScaler()\n",
    "            scaled_train_features = scaler.fit_transform(train_features)\n",
    "        elif scaler_name == 'standard_scaler':\n",
    "            scaler = StandardScaler()\n",
    "            scaled_train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "        # scaled_train_features = tf.keras.layers.Normalization(axis=-1)\n",
    "        # scaled_train_features.adapt(np.array(train_features))\n",
    "\n",
    "        # Create a neural network model\n",
    "        model = keras.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            model.add(layers.Dense(num_neurons, activation=activation, kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "        # model.add(layers.Dense(16))\n",
    "        model.add(layers.Reshape((1,112)))\n",
    "        model.add(layers.SimpleRNN(112, activation='tanh'))\n",
    "        model.add(layers.Dense(3))\n",
    "\n",
    "        # Define the optimizer with the given hyperparameters\n",
    "        if optimizer_name == 'adam':\n",
    "            optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, clipvalue=clipvalue)\n",
    "        elif optimizer_name == 'sgd':\n",
    "            optimizer = SGD(learning_rate=learning_rate, clipvalue=clipvalue)\n",
    "        elif optimizer_name == 'rmsprop':\n",
    "            optimizer = RMSprop(learning_rate=learning_rate, clipvalue=clipvalue)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss=loss_name, optimizer=optimizer, metrics=['accuracy']) #'val_accuracy', , 'loss', 'val_loss'\n",
    "\n",
    "        # Train the model on the training data and validate on a validation split\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=30)\n",
    "        history = model.fit(scaled_train_features, train_labels, validation_split=0.2, verbose=0, epochs=epochs, batch_size=batch_size, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluate the final performance of the model on the validation data\n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "    # Run the hyperparameter tuning\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # Scale the data with the best scaler\n",
    "    if best_params['scaler'] == 'normalizer':\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_train_features = scaler.fit_transform(train_features)\n",
    "    elif best_params['scaler'] == 'standard_scaler':\n",
    "        scaler = StandardScaler()\n",
    "        scaled_train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "    # Create a neural network model with the best hyperparameters\n",
    "    model = keras.Sequential()\n",
    "    for i in range(best_params['num_layers']):\n",
    "        model.add(layers.Dense(best_params['num_neurons'], activation=best_params['activation'], kernel_regularizer=regularizers.l1_l2(l1=best_params['l1'], l2=best_params['l2'])))\n",
    "    # model.add(layers.Dense(16))\n",
    "    model.add(layers.Dense(3))\n",
    "\n",
    "    # Define the optimizer with the best hyperparameters\n",
    "    if best_params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=best_params['learning_rate'], beta_1=best_params['beta_1'], beta_2=best_params['beta_2'], clipvalue=best_params['clipvalue'])\n",
    "    elif best_params['optimizer'] == 'sgd':\n",
    "        optimizer = SGD(learning_rate=best_params['learning_rate'], clipvalue=best_params['clipvalue'])\n",
    "    elif best_params['optimizer'] == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=best_params['learning_rate'], clipvalue=best_params['clipvalue'])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=best_params['loss'], optimizer=optimizer)\n",
    "\n",
    "    return model, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get a list of all H5 files\n",
    "# h5_files = glob.glob('scanner3DTop_Transformations/TESTDATASET_*/*.h5')\n",
    "h5_files = glob.glob('scanner3DTop_Transformations/**/**/**.h5', recursive= True)\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for h5_file in h5_files:\n",
    "\n",
    "    # Open H5 file\n",
    "    h5 = h5py.File(h5_file, 'r')\n",
    "    keys = list(h5.keys())\n",
    "    \n",
    "    if \"EW\" in keys and \"NS\" in keys and \"environment_logger\" in keys:\n",
    "\n",
    "        # Get negative direction transformations\n",
    "        negative = get_group_data(group=h5[\"EW/individual/transformations/negative/\"])\n",
    "\n",
    "        # Get positive direction transformations\n",
    "        positive = get_group_data(group=h5[\"EW/individual/transformations/positive/\"])\n",
    "\n",
    "        # Combine positive and negative transformations\n",
    "        transformations = pd.concat([positive, negative])\n",
    "\n",
    "        # Extract Environment Logger data\n",
    "        df = pd.read_hdf(h5_file, 'environment_logger') \n",
    "\n",
    "        # Merge transformations and Environment logger data\n",
    "        df = df.merge(transformations, on='filename')\n",
    "\n",
    "        # Drop unwanted columns\n",
    "        df = df.drop(['directories', 'filename', 'dt'], axis=1)\n",
    "\n",
    "        # Flatten the transformations\n",
    "        df['data'] = df['data'].apply(lambda x: x.flatten())\n",
    "\n",
    "        df['field'] = df['field'].map({'north': 0.0, 'south': 1.0})\n",
    "        df['scan_direction'] = df['scan_direction'].map({'Negative': 0.0, 'Positive': 1.0})\n",
    "        df = df.drop(['brightness'], axis=1) #, 'time'\n",
    "\n",
    "        # Reset the index of your DataFrame\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        # Add to list\n",
    "        df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract X, Y, Z transformations\n",
    "df['data'] = df['data'].apply(lambda x: x.reshape(4, 4)[:3,3])#[:3])\n",
    "# df['data'] = df['data'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= df.drop(['time', 'par','field', 'x_position', 'y_position', 'z_position'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>sunDirection</th>\n",
       "      <th>airPressure</th>\n",
       "      <th>relHumidity</th>\n",
       "      <th>temperature</th>\n",
       "      <th>windDirection</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>windVelocity</th>\n",
       "      <th>par</th>\n",
       "      <th>field</th>\n",
       "      <th>x_position</th>\n",
       "      <th>y_position</th>\n",
       "      <th>z_position</th>\n",
       "      <th>scan_direction</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-05 23:09:24</td>\n",
       "      <td>338.213446</td>\n",
       "      <td>1010.343944</td>\n",
       "      <td>19.568468</td>\n",
       "      <td>25.102084</td>\n",
       "      <td>173.380535</td>\n",
       "      <td>0.089503</td>\n",
       "      <td>3.887448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>303.8460</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.234</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[-15.0, -15.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-05-05 23:11:12</td>\n",
       "      <td>338.707846</td>\n",
       "      <td>1010.407422</td>\n",
       "      <td>19.681387</td>\n",
       "      <td>25.071566</td>\n",
       "      <td>171.128269</td>\n",
       "      <td>0.089503</td>\n",
       "      <td>3.416852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>304.5450</td>\n",
       "      <td>22.135</td>\n",
       "      <td>1.234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-10.0, 15.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-05-05 23:13:01</td>\n",
       "      <td>339.213233</td>\n",
       "      <td>1010.383618</td>\n",
       "      <td>19.989624</td>\n",
       "      <td>24.931181</td>\n",
       "      <td>171.402936</td>\n",
       "      <td>0.086451</td>\n",
       "      <td>3.394879</td>\n",
       "      <td>1.465023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>305.2460</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.234</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[-20.0, -15.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-05-05 23:14:50</td>\n",
       "      <td>339.707633</td>\n",
       "      <td>1010.296335</td>\n",
       "      <td>19.870602</td>\n",
       "      <td>24.949492</td>\n",
       "      <td>162.470779</td>\n",
       "      <td>0.089503</td>\n",
       "      <td>3.050630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>305.9455</td>\n",
       "      <td>22.135</td>\n",
       "      <td>1.234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-15.0, 10.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-05-05 23:16:38</td>\n",
       "      <td>340.311899</td>\n",
       "      <td>1010.256661</td>\n",
       "      <td>19.965209</td>\n",
       "      <td>24.827418</td>\n",
       "      <td>158.658406</td>\n",
       "      <td>0.086451</td>\n",
       "      <td>3.517563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>306.6455</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.234</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[-10.0, -10.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248</th>\n",
       "      <td>2023-07-21 16:27:18</td>\n",
       "      <td>272.403333</td>\n",
       "      <td>1004.527726</td>\n",
       "      <td>16.840114</td>\n",
       "      <td>44.298227</td>\n",
       "      <td>268.008667</td>\n",
       "      <td>0.110866</td>\n",
       "      <td>6.912442</td>\n",
       "      <td>1120.253937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>304.5460</td>\n",
       "      <td>22.135</td>\n",
       "      <td>2.319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[115.0, 85.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>2023-07-21 16:29:13</td>\n",
       "      <td>272.721946</td>\n",
       "      <td>1004.511856</td>\n",
       "      <td>17.099521</td>\n",
       "      <td>44.578997</td>\n",
       "      <td>273.732719</td>\n",
       "      <td>0.104762</td>\n",
       "      <td>4.070559</td>\n",
       "      <td>1103.162007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>305.2460</td>\n",
       "      <td>3.800</td>\n",
       "      <td>2.319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[120.0, 30.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>2023-07-21 16:31:07</td>\n",
       "      <td>272.919706</td>\n",
       "      <td>1004.503922</td>\n",
       "      <td>17.221595</td>\n",
       "      <td>44.548479</td>\n",
       "      <td>265.437788</td>\n",
       "      <td>0.104762</td>\n",
       "      <td>6.286203</td>\n",
       "      <td>1090.953486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>305.9460</td>\n",
       "      <td>22.135</td>\n",
       "      <td>2.319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[25.0, -15.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>2023-07-21 16:33:00</td>\n",
       "      <td>273.216346</td>\n",
       "      <td>1004.456313</td>\n",
       "      <td>17.334513</td>\n",
       "      <td>44.426405</td>\n",
       "      <td>269.436934</td>\n",
       "      <td>0.104762</td>\n",
       "      <td>4.409314</td>\n",
       "      <td>1073.373215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>306.6455</td>\n",
       "      <td>3.800</td>\n",
       "      <td>2.319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[390.0, 250.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252</th>\n",
       "      <td>2023-07-21 16:34:56</td>\n",
       "      <td>273.414106</td>\n",
       "      <td>1004.392834</td>\n",
       "      <td>17.447432</td>\n",
       "      <td>44.246345</td>\n",
       "      <td>288.586688</td>\n",
       "      <td>0.104762</td>\n",
       "      <td>4.477065</td>\n",
       "      <td>1069.466488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>307.3460</td>\n",
       "      <td>22.135</td>\n",
       "      <td>2.319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[85.0, 50.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4253 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    time  sunDirection  airPressure  relHumidity  temperature  \\\n",
       "0    2022-05-05 23:09:24    338.213446  1010.343944    19.568468    25.102084   \n",
       "1    2022-05-05 23:11:12    338.707846  1010.407422    19.681387    25.071566   \n",
       "2    2022-05-05 23:13:01    339.213233  1010.383618    19.989624    24.931181   \n",
       "3    2022-05-05 23:14:50    339.707633  1010.296335    19.870602    24.949492   \n",
       "4    2022-05-05 23:16:38    340.311899  1010.256661    19.965209    24.827418   \n",
       "...                  ...           ...          ...          ...          ...   \n",
       "4248 2023-07-21 16:27:18    272.403333  1004.527726    16.840114    44.298227   \n",
       "4249 2023-07-21 16:29:13    272.721946  1004.511856    17.099521    44.578997   \n",
       "4250 2023-07-21 16:31:07    272.919706  1004.503922    17.221595    44.548479   \n",
       "4251 2023-07-21 16:33:00    273.216346  1004.456313    17.334513    44.426405   \n",
       "4252 2023-07-21 16:34:56    273.414106  1004.392834    17.447432    44.246345   \n",
       "\n",
       "      windDirection  precipitation  windVelocity          par  field  \\\n",
       "0        173.380535       0.089503      3.887448     0.000000    0.0   \n",
       "1        171.128269       0.089503      3.416852     0.000000    0.0   \n",
       "2        171.402936       0.086451      3.394879     1.465023    0.0   \n",
       "3        162.470779       0.089503      3.050630     0.000000    0.0   \n",
       "4        158.658406       0.086451      3.517563     0.000000    0.0   \n",
       "...             ...            ...           ...          ...    ...   \n",
       "4248     268.008667       0.110866      6.912442  1120.253937    0.0   \n",
       "4249     273.732719       0.104762      4.070559  1103.162007    0.0   \n",
       "4250     265.437788       0.104762      6.286203  1090.953486    0.0   \n",
       "4251     269.436934       0.104762      4.409314  1073.373215    0.0   \n",
       "4252     288.586688       0.104762      4.477065  1069.466488    0.0   \n",
       "\n",
       "      x_position  y_position  z_position  scan_direction                 data  \n",
       "0       303.8460       0.000       1.234             1.0  [-15.0, -15.0, 0.0]  \n",
       "1       304.5450      22.135       1.234             0.0   [-10.0, 15.0, 0.0]  \n",
       "2       305.2460       0.000       1.234             1.0  [-20.0, -15.0, 0.0]  \n",
       "3       305.9455      22.135       1.234             0.0   [-15.0, 10.0, 0.0]  \n",
       "4       306.6455       0.000       1.234             1.0  [-10.0, -10.0, 0.0]  \n",
       "...          ...         ...         ...             ...                  ...  \n",
       "4248    304.5460      22.135       2.319             0.0   [115.0, 85.0, 0.0]  \n",
       "4249    305.2460       3.800       2.319             1.0   [120.0, 30.0, 0.0]  \n",
       "4250    305.9460      22.135       2.319             0.0   [25.0, -15.0, 0.0]  \n",
       "4251    306.6455       3.800       2.319             1.0  [390.0, 250.0, 0.0]  \n",
       "4252    307.3460      22.135       2.319             0.0    [85.0, 50.0, 0.0]  \n",
       "\n",
       "[4253 rows x 15 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df2\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "\n",
    "# Convert any Timestamps in the input features to a numerical representation\n",
    "# dataset = dataset.apply(lambda x: x.astype(int) if np.issubdtype(x.dtype, np.datetime64) else x)\n",
    "#dataset['time'] = dataset['time'].values.astype(int)\n",
    "\n",
    "# Convert only the columns with numeric data types to float\n",
    "#dataset[dataset.select_dtypes(include='number').columns] = dataset.select_dtypes(include='number').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "train_labels = train_features.pop('data')\n",
    "test_labels = test_features.pop('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels.tolist()\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_labels = test_labels.tolist()\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow.keras.backend as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3402, 8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-11 04:43:29,715] A new study created in memory with name: no-name-4bbadd99-d8ae-4730-83d0-46d7a6dd41ff\n",
      "[I 2024-01-11 04:48:55,121] Trial 0 finished with value: 8.481119155883789 and parameters: {}. Best is trial 0 with value: 8.481119155883789.\n",
      "[I 2024-01-11 04:56:19,509] Trial 1 finished with value: 8.60782241821289 and parameters: {'learning_rate': 0.009343980237730289}. Best is trial 0 with value: 8.481119155883789.\n",
      "[I 2024-01-11 05:03:45,001] Trial 2 finished with value: 8.68367862701416 and parameters: {'learning_rate': 0.0011346746561896515}. Best is trial 0 with value: 8.481119155883789.\n",
      "[I 2024-01-11 05:11:15,377] Trial 3 finished with value: 8.198264122009277 and parameters: {'learning_rate': 0.10514088102386045}. Best is trial 3 with value: 8.198264122009277.\n",
      "[I 2024-01-11 05:15:41,606] Trial 4 finished with value: 11.535700798034668 and parameters: {'learning_rate': 0.007908072847787745}. Best is trial 3 with value: 8.198264122009277.\n",
      "[I 2024-01-11 05:22:57,620] Trial 5 finished with value: 9.450066566467285 and parameters: {'learning_rate': 3.689479528062541e-05}. Best is trial 3 with value: 8.198264122009277.\n",
      "[I 2024-01-11 05:28:36,426] Trial 6 finished with value: 10.315497398376465 and parameters: {'learning_rate': 0.0007910495254172824}. Best is trial 3 with value: 8.198264122009277.\n",
      "[I 2024-01-11 05:36:49,586] Trial 7 finished with value: 8.40864086151123 and parameters: {'learning_rate': 0.08387264505824722}. Best is trial 3 with value: 8.198264122009277.\n",
      "[I 2024-01-11 05:44:18,648] Trial 8 finished with value: 8.883163452148438 and parameters: {'learning_rate': 9.467346819813051e-05}. Best is trial 3 with value: 8.198264122009277.\n",
      "[I 2024-01-11 05:49:31,702] Trial 9 finished with value: 9.402374267578125 and parameters: {'learning_rate': 0.0819476913421377}. Best is trial 3 with value: 8.198264122009277.\n",
      "[I 2024-01-11 05:57:05,885] Trial 10 finished with value: 8.878761291503906 and parameters: {'learning_rate': 0.40892589462875717}. Best is trial 3 with value: 8.198264122009277.\n",
      "[I 2024-01-11 06:04:37,812] Trial 11 finished with value: 8.802447319030762 and parameters: {'learning_rate': 0.8778948822052189}. Best is trial 3 with value: 8.198264122009277.\n",
      "[I 2024-01-11 06:12:10,311] Trial 12 finished with value: 8.154003143310547 and parameters: {'learning_rate': 0.10941961020860373}. Best is trial 12 with value: 8.154003143310547.\n",
      "[I 2024-01-11 06:19:47,355] Trial 13 finished with value: 9.401576042175293 and parameters: {'learning_rate': 0.08052890847948149}. Best is trial 12 with value: 8.154003143310547.\n",
      "[I 2024-01-11 06:27:08,547] Trial 14 finished with value: 9.471539497375488 and parameters: {'learning_rate': 0.027067262160562065}. Best is trial 12 with value: 8.154003143310547.\n",
      "[I 2024-01-11 06:34:42,748] Trial 15 finished with value: 8.400734901428223 and parameters: {'learning_rate': 0.39758726555078105}. Best is trial 12 with value: 8.154003143310547.\n",
      "[I 2024-01-11 06:42:12,664] Trial 16 finished with value: 8.513022422790527 and parameters: {'learning_rate': 0.02861804952170448}. Best is trial 12 with value: 8.154003143310547.\n",
      "[I 2024-01-11 06:49:48,494] Trial 17 finished with value: 8.609264373779297 and parameters: {'learning_rate': 0.17440924553550644}. Best is trial 12 with value: 8.154003143310547.\n",
      "[I 2024-01-11 06:56:46,410] Trial 18 finished with value: 8.479595184326172 and parameters: {'learning_rate': 0.9756041697281365}. Best is trial 12 with value: 8.154003143310547.\n",
      "[I 2024-01-11 07:02:55,467] Trial 19 finished with value: 9.134700775146484 and parameters: {'learning_rate': 0.024058064181391912}. Best is trial 12 with value: 8.154003143310547.\n",
      "[I 2024-01-11 07:10:26,043] Trial 20 finished with value: 8.777544975280762 and parameters: {'learning_rate': 0.0040792172451978494}. Best is trial 12 with value: 8.154003143310547.\n",
      "[I 2024-01-11 07:17:57,822] Trial 21 finished with value: 9.237127304077148 and parameters: {'learning_rate': 0.2727545331640011}. Best is trial 12 with value: 8.154003143310547.\n",
      "[I 2024-01-11 07:25:24,967] Trial 22 finished with value: 8.750618934631348 and parameters: {'learning_rate': 0.2843149595765905}. Best is trial 12 with value: 8.154003143310547.\n",
      "[I 2024-01-11 07:32:28,860] Trial 23 finished with value: 8.841961860656738 and parameters: {'learning_rate': 0.14980068427336754}. Best is trial 12 with value: 8.154003143310547.\n",
      "[I 2024-01-11 07:40:24,063] Trial 24 finished with value: 8.021158218383789 and parameters: {'learning_rate': 0.5618520474090932}. Best is trial 24 with value: 8.021158218383789.\n",
      "[I 2024-01-11 07:47:46,585] Trial 25 finished with value: 9.157146453857422 and parameters: {'learning_rate': 0.952604115902035}. Best is trial 24 with value: 8.021158218383789.\n",
      "[I 2024-01-11 07:55:00,025] Trial 26 finished with value: 8.890217781066895 and parameters: {'learning_rate': 0.04754314165665494}. Best is trial 24 with value: 8.021158218383789.\n",
      "[I 2024-01-11 08:00:19,527] Trial 27 finished with value: 9.985589981079102 and parameters: {'learning_rate': 0.12566326233872863}. Best is trial 24 with value: 8.021158218383789.\n",
      "[I 2024-01-11 08:08:06,829] Trial 28 finished with value: 8.37183952331543 and parameters: {'learning_rate': 0.41436388967563514}. Best is trial 24 with value: 8.021158218383789.\n",
      "[I 2024-01-11 08:14:31,563] Trial 29 finished with value: 9.034659385681152 and parameters: {'learning_rate': 0.048247877790484214}. Best is trial 24 with value: 8.021158218383789.\n",
      "[I 2024-01-11 08:22:09,728] Trial 30 finished with value: 8.871295928955078 and parameters: {'learning_rate': 0.014164816857250366}. Best is trial 24 with value: 8.021158218383789.\n",
      "[I 2024-01-11 08:29:45,666] Trial 31 finished with value: 8.501518249511719 and parameters: {'learning_rate': 0.4674898665666377}. Best is trial 24 with value: 8.021158218383789.\n",
      "[I 2024-01-11 08:34:31,653] Trial 32 finished with value: 10.43207836151123 and parameters: {'learning_rate': 0.21585466694382785}. Best is trial 24 with value: 8.021158218383789.\n",
      "[I 2024-01-11 08:42:53,112] Trial 33 finished with value: 8.993526458740234 and parameters: {'learning_rate': 0.5220921234308193}. Best is trial 24 with value: 8.021158218383789.\n",
      "[I 2024-01-11 08:48:03,767] Trial 34 finished with value: 9.76996898651123 and parameters: {'learning_rate': 0.17124022253937626}. Best is trial 24 with value: 8.021158218383789.\n",
      "[I 2024-01-11 09:25:12,824] Trial 35 finished with value: 9.798534393310547 and parameters: {'learning_rate': 0.5138143744639541}. Best is trial 24 with value: 8.021158218383789.\n",
      "[I 2024-01-11 09:34:45,325] Trial 36 finished with value: 8.588139533996582 and parameters: {'learning_rate': 0.08845766409697951}. Best is trial 24 with value: 8.021158218383789.\n"
     ]
    }
   ],
   "source": [
    "dnn_model, best_parameters = build_and_compile_model(train_features=train_features, train_labels=train_labels, n_trials=100)\n",
    "dnn_model.build(input_shape=(None, train_features.shape[1]))\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data with the best scaler\n",
    "if best_parameters['scaler'] == 'normalizer':\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_train_features = scaler.fit_transform(train_features)\n",
    "elif best_parameters['scaler'] == 'standard_scaler':\n",
    "    scaler = StandardScaler()\n",
    "    scaled_train_features = scaler.fit_transform(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/850\n",
      "15/15 [==============================] - 1s 33ms/step - loss: 31.9655 - val_loss: 33.7600\n",
      "Epoch 2/850\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 30.8660 - val_loss: 32.6985\n",
      "Epoch 3/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 30.4115 - val_loss: 32.4448\n",
      "Epoch 4/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 30.1802 - val_loss: 31.8941\n",
      "Epoch 5/850\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 29.6975 - val_loss: 31.4118\n",
      "Epoch 6/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 29.3307 - val_loss: 31.0647\n",
      "Epoch 7/850\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 29.0517 - val_loss: 30.8022\n",
      "Epoch 8/850\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 28.7853 - val_loss: 30.6172\n",
      "Epoch 9/850\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 28.5324 - val_loss: 30.4550\n",
      "Epoch 10/850\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 28.3704 - val_loss: 30.0634\n",
      "Epoch 11/850\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 28.1290 - val_loss: 30.0441\n",
      "Epoch 12/850\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.9478 - val_loss: 29.8585\n",
      "Epoch 13/850\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.6815 - val_loss: 29.5532\n",
      "Epoch 14/850\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.4479 - val_loss: 29.3652\n",
      "Epoch 15/850\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.1852 - val_loss: 29.0594\n",
      "Epoch 16/850\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.0221 - val_loss: 28.9392\n",
      "Epoch 17/850\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 26.7852 - val_loss: 28.5529\n",
      "Epoch 18/850\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 26.5718 - val_loss: 28.1904\n",
      "Epoch 19/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26.3051 - val_loss: 28.0402\n",
      "Epoch 20/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26.0031 - val_loss: 27.9689\n",
      "Epoch 21/850\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.8552 - val_loss: 27.2217\n",
      "Epoch 22/850\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.5001 - val_loss: 26.8509\n",
      "Epoch 23/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 25.2148 - val_loss: 26.7004\n",
      "Epoch 24/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 24.9703 - val_loss: 26.5576\n",
      "Epoch 25/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.6248 - val_loss: 26.0851\n",
      "Epoch 26/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.1783 - val_loss: 25.9633\n",
      "Epoch 27/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 23.9505 - val_loss: 25.6594\n",
      "Epoch 28/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 23.6859 - val_loss: 25.7950\n",
      "Epoch 29/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 23.5371 - val_loss: 25.5728\n",
      "Epoch 30/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 23.1028 - val_loss: 25.1417\n",
      "Epoch 31/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 22.8916 - val_loss: 25.1011\n",
      "Epoch 32/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 22.6372 - val_loss: 24.8949\n",
      "Epoch 33/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 22.2286 - val_loss: 24.5082\n",
      "Epoch 34/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 21.8811 - val_loss: 24.1102\n",
      "Epoch 35/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 21.5073 - val_loss: 24.1103\n",
      "Epoch 36/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 21.4330 - val_loss: 23.6856\n",
      "Epoch 37/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 21.1245 - val_loss: 23.7672\n",
      "Epoch 38/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 20.6949 - val_loss: 23.2375\n",
      "Epoch 39/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 20.4239 - val_loss: 23.1302\n",
      "Epoch 40/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 20.1678 - val_loss: 22.9532\n",
      "Epoch 41/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 19.6657 - val_loss: 22.9603\n",
      "Epoch 42/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 19.2939 - val_loss: 22.5290\n",
      "Epoch 43/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 19.0341 - val_loss: 22.3046\n",
      "Epoch 44/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 19.0326 - val_loss: 22.4419\n",
      "Epoch 45/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.8338 - val_loss: 22.2491\n",
      "Epoch 46/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.5896 - val_loss: 21.8688\n",
      "Epoch 47/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 18.1898 - val_loss: 21.4982\n",
      "Epoch 48/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.1789 - val_loss: 21.7033\n",
      "Epoch 49/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 17.9195 - val_loss: 21.3409\n",
      "Epoch 50/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 17.6849 - val_loss: 21.4858\n",
      "Epoch 51/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 17.5159 - val_loss: 21.1213\n",
      "Epoch 52/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 17.2705 - val_loss: 20.8627\n",
      "Epoch 53/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 17.0451 - val_loss: 21.0019\n",
      "Epoch 54/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.8377 - val_loss: 20.5470\n",
      "Epoch 55/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.6311 - val_loss: 20.2772\n",
      "Epoch 56/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.5507 - val_loss: 20.4475\n",
      "Epoch 57/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 16.3752 - val_loss: 20.5332\n",
      "Epoch 58/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 16.2606 - val_loss: 20.2773\n",
      "Epoch 59/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 16.1503 - val_loss: 19.9574\n",
      "Epoch 60/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.5607 - val_loss: 19.5100\n",
      "Epoch 61/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.4404 - val_loss: 19.9172\n",
      "Epoch 62/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.2195 - val_loss: 19.2631\n",
      "Epoch 63/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.0984 - val_loss: 19.1251\n",
      "Epoch 64/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.0282 - val_loss: 19.3662\n",
      "Epoch 65/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 14.6777 - val_loss: 18.8618\n",
      "Epoch 66/850\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 14.6886 - val_loss: 19.7650\n",
      "Epoch 67/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 14.5269 - val_loss: 18.8262\n",
      "Epoch 68/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4485 - val_loss: 18.9852\n",
      "Epoch 69/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.0915 - val_loss: 18.5023\n",
      "Epoch 70/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.7250 - val_loss: 18.4656\n",
      "Epoch 71/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.7707 - val_loss: 17.9684\n",
      "Epoch 72/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.7535 - val_loss: 17.9629\n",
      "Epoch 73/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.2957 - val_loss: 17.9678\n",
      "Epoch 74/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.6941 - val_loss: 17.8097\n",
      "Epoch 75/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 13.1745 - val_loss: 17.7336\n",
      "Epoch 76/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.9396 - val_loss: 17.3668\n",
      "Epoch 77/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.0422 - val_loss: 17.0686\n",
      "Epoch 78/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.6671 - val_loss: 17.1073\n",
      "Epoch 79/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.3981 - val_loss: 16.5304\n",
      "Epoch 80/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.3896 - val_loss: 16.6734\n",
      "Epoch 81/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.1108 - val_loss: 16.7430\n",
      "Epoch 82/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.9972 - val_loss: 16.8327\n",
      "Epoch 83/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.4575 - val_loss: 16.0315\n",
      "Epoch 84/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.6809 - val_loss: 16.1021\n",
      "Epoch 85/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.7659 - val_loss: 16.5930\n",
      "Epoch 86/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.4758 - val_loss: 15.8038\n",
      "Epoch 87/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.6792 - val_loss: 16.0672\n",
      "Epoch 88/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.3995 - val_loss: 15.6710\n",
      "Epoch 89/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.4859 - val_loss: 15.5422\n",
      "Epoch 90/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.0964 - val_loss: 15.5663\n",
      "Epoch 91/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.0777 - val_loss: 15.4616\n",
      "Epoch 92/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 10.9695 - val_loss: 14.9745\n",
      "Epoch 93/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 10.7443 - val_loss: 14.6595\n",
      "Epoch 94/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 10.7099 - val_loss: 14.4494\n",
      "Epoch 95/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 10.8095 - val_loss: 14.6979\n",
      "Epoch 96/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 10.6275 - val_loss: 14.3944\n",
      "Epoch 97/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 10.3340 - val_loss: 14.0176\n",
      "Epoch 98/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 10.4243 - val_loss: 14.7954\n",
      "Epoch 99/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 10.2228 - val_loss: 14.4739\n",
      "Epoch 100/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 10.1730 - val_loss: 13.6951\n",
      "Epoch 101/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 10.0794 - val_loss: 13.8546\n",
      "Epoch 102/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 9.8251 - val_loss: 14.0685\n",
      "Epoch 103/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 9.8637 - val_loss: 13.6828\n",
      "Epoch 104/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 9.9269 - val_loss: 13.2852\n",
      "Epoch 105/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 9.6529 - val_loss: 13.2891\n",
      "Epoch 106/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 9.5668 - val_loss: 13.0677\n",
      "Epoch 107/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 9.6159 - val_loss: 13.1128\n",
      "Epoch 108/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 9.3671 - val_loss: 12.9448\n",
      "Epoch 109/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 9.2503 - val_loss: 13.1489\n",
      "Epoch 110/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 9.6258 - val_loss: 13.3132\n",
      "Epoch 111/850\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 9.1352 - val_loss: 12.3169\n",
      "Epoch 112/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 9.0285 - val_loss: 12.4639\n",
      "Epoch 113/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 9.1178 - val_loss: 12.8190\n",
      "Epoch 114/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 8.8615 - val_loss: 12.5193\n",
      "Epoch 115/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 8.9933 - val_loss: 12.6198\n",
      "Epoch 116/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 8.9662 - val_loss: 12.0609\n",
      "Epoch 117/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 8.7293 - val_loss: 12.4330\n",
      "Epoch 118/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 8.8674 - val_loss: 12.7137\n",
      "Epoch 119/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 8.7725 - val_loss: 12.3511\n",
      "Epoch 120/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 8.5540 - val_loss: 12.9855\n",
      "Epoch 121/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 8.8702 - val_loss: 11.9246\n",
      "Epoch 122/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 8.5809 - val_loss: 12.4123\n",
      "Epoch 123/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 8.5165 - val_loss: 12.1542\n",
      "Epoch 124/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 8.3556 - val_loss: 11.8025\n",
      "Epoch 125/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 8.4014 - val_loss: 12.0424\n",
      "Epoch 126/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 8.3558 - val_loss: 11.8461\n",
      "Epoch 127/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 8.0862 - val_loss: 11.4145\n",
      "Epoch 128/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 8.2416 - val_loss: 11.9803\n",
      "Epoch 129/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 8.3041 - val_loss: 11.5614\n",
      "Epoch 130/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 8.1610 - val_loss: 11.9183\n",
      "Epoch 131/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 8.1165 - val_loss: 12.1803\n",
      "Epoch 132/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 8.2572 - val_loss: 11.9300\n",
      "Epoch 133/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 8.0803 - val_loss: 11.4334\n",
      "Epoch 134/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 7.6582 - val_loss: 11.3009\n",
      "Epoch 135/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 7.7005 - val_loss: 11.6079\n",
      "Epoch 136/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 7.7333 - val_loss: 11.6106\n",
      "Epoch 137/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 7.9057 - val_loss: 11.3043\n",
      "Epoch 138/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 7.8023 - val_loss: 11.0533\n",
      "Epoch 139/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 7.8556 - val_loss: 10.9283\n",
      "Epoch 140/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 7.3482 - val_loss: 11.2493\n",
      "Epoch 141/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 7.3982 - val_loss: 11.1087\n",
      "Epoch 142/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 7.5042 - val_loss: 10.9509\n",
      "Epoch 143/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 7.4203 - val_loss: 10.8979\n",
      "Epoch 144/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 7.5782 - val_loss: 11.3366\n",
      "Epoch 145/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 7.5729 - val_loss: 11.0449\n",
      "Epoch 146/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 7.2852 - val_loss: 11.0976\n",
      "Epoch 147/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 7.1389 - val_loss: 10.4431\n",
      "Epoch 148/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 7.1493 - val_loss: 10.7663\n",
      "Epoch 149/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 6.9161 - val_loss: 10.4222\n",
      "Epoch 150/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 6.9058 - val_loss: 10.1223\n",
      "Epoch 151/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8953 - val_loss: 10.0850\n",
      "Epoch 152/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 6.8172 - val_loss: 10.2985\n",
      "Epoch 153/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 7.1813 - val_loss: 10.1167\n",
      "Epoch 154/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 7.0445 - val_loss: 10.0589\n",
      "Epoch 155/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.7613 - val_loss: 9.8656\n",
      "Epoch 156/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.6158 - val_loss: 10.1880\n",
      "Epoch 157/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8161 - val_loss: 10.0166\n",
      "Epoch 158/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.7748 - val_loss: 9.9851\n",
      "Epoch 159/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.7021 - val_loss: 9.8974\n",
      "Epoch 160/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.7377 - val_loss: 9.8144\n",
      "Epoch 161/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.6274 - val_loss: 9.8017\n",
      "Epoch 162/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.7702 - val_loss: 9.8041\n",
      "Epoch 163/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 6.6282 - val_loss: 9.7894\n",
      "Epoch 164/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.5860 - val_loss: 10.1250\n",
      "Epoch 165/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.5916 - val_loss: 9.5877\n",
      "Epoch 166/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.5050 - val_loss: 9.5819\n",
      "Epoch 167/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.4031 - val_loss: 9.3856\n",
      "Epoch 168/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.3522 - val_loss: 9.0709\n",
      "Epoch 169/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.3746 - val_loss: 9.8994\n",
      "Epoch 170/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.4365 - val_loss: 9.8126\n",
      "Epoch 171/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.3961 - val_loss: 9.3884\n",
      "Epoch 172/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.2203 - val_loss: 9.1793\n",
      "Epoch 173/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.1547 - val_loss: 9.2344\n",
      "Epoch 174/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 6.3271 - val_loss: 9.6814\n",
      "Epoch 175/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 6.4289 - val_loss: 9.2794\n",
      "Epoch 176/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.1180 - val_loss: 9.5672\n",
      "Epoch 177/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.0704 - val_loss: 10.0209\n",
      "Epoch 178/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.1738 - val_loss: 9.3015\n",
      "Epoch 179/850\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 6.0891 - val_loss: 9.7200\n",
      "Epoch 180/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 6.1153 - val_loss: 8.8313\n",
      "Epoch 181/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.8365 - val_loss: 8.7284\n",
      "Epoch 182/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.8803 - val_loss: 8.9718\n",
      "Epoch 183/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.0808 - val_loss: 9.1280\n",
      "Epoch 184/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 6.1508 - val_loss: 8.9828\n",
      "Epoch 185/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.8593 - val_loss: 8.8929\n",
      "Epoch 186/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.8559 - val_loss: 9.1867\n",
      "Epoch 187/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.9303 - val_loss: 9.6790\n",
      "Epoch 188/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.3958 - val_loss: 8.9802\n",
      "Epoch 189/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.7523 - val_loss: 8.5677\n",
      "Epoch 190/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.7877 - val_loss: 8.6313\n",
      "Epoch 191/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.7293 - val_loss: 8.7768\n",
      "Epoch 192/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.7260 - val_loss: 9.0198\n",
      "Epoch 193/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.6421 - val_loss: 9.0675\n",
      "Epoch 194/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.7458 - val_loss: 8.8732\n",
      "Epoch 195/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.7493 - val_loss: 8.8673\n",
      "Epoch 196/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.7256 - val_loss: 8.5862\n",
      "Epoch 197/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 5.5686 - val_loss: 8.7539\n",
      "Epoch 198/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 5.6880 - val_loss: 8.6290\n",
      "Epoch 199/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.4752 - val_loss: 8.6646\n",
      "Epoch 200/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 5.7146 - val_loss: 8.5688\n",
      "Epoch 201/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 5.7164 - val_loss: 8.8656\n",
      "Epoch 202/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 5.7593 - val_loss: 8.7643\n",
      "Epoch 203/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 5.6754 - val_loss: 8.9101\n",
      "Epoch 204/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.4126 - val_loss: 8.3158\n",
      "Epoch 205/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.4087 - val_loss: 8.2639\n",
      "Epoch 206/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.5970 - val_loss: 8.5269\n",
      "Epoch 207/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.4418 - val_loss: 8.4843\n",
      "Epoch 208/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.3543 - val_loss: 8.4246\n",
      "Epoch 209/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.2952 - val_loss: 8.3349\n",
      "Epoch 210/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.2926 - val_loss: 8.5154\n",
      "Epoch 211/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.3432 - val_loss: 8.1961\n",
      "Epoch 212/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 5.3222 - val_loss: 8.6646\n",
      "Epoch 213/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.6036 - val_loss: 8.2859\n",
      "Epoch 214/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.4300 - val_loss: 8.2722\n",
      "Epoch 215/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.2500 - val_loss: 8.0900\n",
      "Epoch 216/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.2147 - val_loss: 8.0045\n",
      "Epoch 217/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.1307 - val_loss: 8.4080\n",
      "Epoch 218/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.2941 - val_loss: 8.2368\n",
      "Epoch 219/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.3391 - val_loss: 8.2793\n",
      "Epoch 220/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.1739 - val_loss: 7.8525\n",
      "Epoch 221/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 4.9856 - val_loss: 8.3778\n",
      "Epoch 222/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.2857 - val_loss: 8.1967\n",
      "Epoch 223/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.1857 - val_loss: 7.9005\n",
      "Epoch 224/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.3083 - val_loss: 8.7394\n",
      "Epoch 225/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.2287 - val_loss: 8.5630\n",
      "Epoch 226/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.6121 - val_loss: 8.3993\n",
      "Epoch 227/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.2541 - val_loss: 7.7538\n",
      "Epoch 228/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.9978 - val_loss: 7.8936\n",
      "Epoch 229/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.8356 - val_loss: 7.7127\n",
      "Epoch 230/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.3619 - val_loss: 7.7216\n",
      "Epoch 231/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4.9837 - val_loss: 8.3994\n",
      "Epoch 232/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4.9908 - val_loss: 8.0078\n",
      "Epoch 233/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.0357 - val_loss: 7.8696\n",
      "Epoch 234/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.1886 - val_loss: 8.0396\n",
      "Epoch 235/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.0999 - val_loss: 8.2902\n",
      "Epoch 236/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.1773 - val_loss: 7.9408\n",
      "Epoch 237/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.1593 - val_loss: 8.8782\n",
      "Epoch 238/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 5.2070 - val_loss: 8.2128\n",
      "Epoch 239/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 5.0678 - val_loss: 8.3372\n",
      "Epoch 240/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.0005 - val_loss: 7.8716\n",
      "Epoch 241/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.9791 - val_loss: 8.2653\n",
      "Epoch 242/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.9618 - val_loss: 7.9353\n",
      "Epoch 243/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.7732 - val_loss: 7.6053\n",
      "Epoch 244/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 4.7876 - val_loss: 7.8089\n",
      "Epoch 245/850\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 5.0022 - val_loss: 7.9778\n",
      "Epoch 246/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.8771 - val_loss: 8.3764\n",
      "Epoch 247/850\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4.8207 - val_loss: 8.0139\n",
      "Epoch 248/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5.0205 - val_loss: 8.0433\n",
      "Epoch 249/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.8168 - val_loss: 7.6110\n",
      "Epoch 250/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.7174 - val_loss: 7.6317\n",
      "Epoch 251/850\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4.6053 - val_loss: 7.8720\n",
      "Epoch 252/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.8090 - val_loss: 7.5581\n",
      "Epoch 253/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4.8755 - val_loss: 8.1271\n",
      "Epoch 254/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 4.9213 - val_loss: 7.7581\n",
      "Epoch 255/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.6703 - val_loss: 7.3665\n",
      "Epoch 256/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.5733 - val_loss: 7.4790\n",
      "Epoch 257/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.4958 - val_loss: 7.6636\n",
      "Epoch 258/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.5773 - val_loss: 7.6605\n",
      "Epoch 259/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.6217 - val_loss: 7.4204\n",
      "Epoch 260/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.4885 - val_loss: 7.5831\n",
      "Epoch 261/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.6445 - val_loss: 7.6263\n",
      "Epoch 262/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.6945 - val_loss: 7.3234\n",
      "Epoch 263/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.5739 - val_loss: 7.3784\n",
      "Epoch 264/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.5737 - val_loss: 7.8040\n",
      "Epoch 265/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.7098 - val_loss: 7.3964\n",
      "Epoch 266/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.4924 - val_loss: 7.2937\n",
      "Epoch 267/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.4089 - val_loss: 7.2946\n",
      "Epoch 268/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.5210 - val_loss: 7.3964\n",
      "Epoch 269/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.6047 - val_loss: 7.5587\n",
      "Epoch 270/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.5349 - val_loss: 7.2182\n",
      "Epoch 271/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.4306 - val_loss: 7.1141\n",
      "Epoch 272/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.3200 - val_loss: 7.5761\n",
      "Epoch 273/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.4257 - val_loss: 7.5621\n",
      "Epoch 274/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.5295 - val_loss: 7.5529\n",
      "Epoch 275/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.3599 - val_loss: 7.0396\n",
      "Epoch 276/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.2770 - val_loss: 7.2800\n",
      "Epoch 277/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4.3245 - val_loss: 7.1456\n",
      "Epoch 278/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4.3356 - val_loss: 7.0994\n",
      "Epoch 279/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.5482 - val_loss: 7.6036\n",
      "Epoch 280/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.4180 - val_loss: 7.0929\n",
      "Epoch 281/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.2767 - val_loss: 7.1449\n",
      "Epoch 282/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.4329 - val_loss: 7.4822\n",
      "Epoch 283/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4.3834 - val_loss: 7.0313\n",
      "Epoch 284/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.3620 - val_loss: 7.6546\n",
      "Epoch 285/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.3970 - val_loss: 7.0747\n",
      "Epoch 286/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 4.2970 - val_loss: 7.3812\n",
      "Epoch 287/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4.2000 - val_loss: 6.9769\n",
      "Epoch 288/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.2154 - val_loss: 7.0411\n",
      "Epoch 289/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4.5053 - val_loss: 7.1876\n",
      "Epoch 290/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.2153 - val_loss: 7.2157\n",
      "Epoch 291/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 4.3086 - val_loss: 7.4580\n",
      "Epoch 292/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4.1951 - val_loss: 7.2089\n",
      "Epoch 293/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.4771 - val_loss: 8.3145\n",
      "Epoch 294/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4.6180 - val_loss: 7.0562\n",
      "Epoch 295/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.1733 - val_loss: 7.0990\n",
      "Epoch 296/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.1801 - val_loss: 7.0628\n",
      "Epoch 297/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4.1265 - val_loss: 7.3905\n",
      "Epoch 298/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.2913 - val_loss: 6.9757\n",
      "Epoch 299/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 4.5084 - val_loss: 7.2991\n",
      "Epoch 300/850\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 4.4334 - val_loss: 7.2515\n",
      "Epoch 301/850\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4.3350 - val_loss: 7.8713\n",
      "Epoch 302/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.3483 - val_loss: 7.3157\n",
      "Epoch 303/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.4442 - val_loss: 7.2042\n",
      "Epoch 304/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4.1667 - val_loss: 6.8888\n",
      "Epoch 305/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4.0263 - val_loss: 6.9953\n",
      "Epoch 306/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.1216 - val_loss: 7.2502\n",
      "Epoch 307/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.0904 - val_loss: 7.0822\n",
      "Epoch 308/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.2179 - val_loss: 6.8769\n",
      "Epoch 309/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.9544 - val_loss: 6.7997\n",
      "Epoch 310/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.9710 - val_loss: 6.8701\n",
      "Epoch 311/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.0171 - val_loss: 6.8338\n",
      "Epoch 312/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.3131 - val_loss: 7.0595\n",
      "Epoch 313/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.0565 - val_loss: 6.8329\n",
      "Epoch 314/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.9485 - val_loss: 7.0372\n",
      "Epoch 315/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.0165 - val_loss: 7.0484\n",
      "Epoch 316/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.0228 - val_loss: 7.2285\n",
      "Epoch 317/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4.1170 - val_loss: 7.2126\n",
      "Epoch 318/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.9661 - val_loss: 6.5563\n",
      "Epoch 319/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.9444 - val_loss: 6.8833\n",
      "Epoch 320/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 4.0812 - val_loss: 6.7908\n",
      "Epoch 321/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.0417 - val_loss: 6.8310\n",
      "Epoch 322/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.9266 - val_loss: 6.9619\n",
      "Epoch 323/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.0320 - val_loss: 6.8751\n",
      "Epoch 324/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.0019 - val_loss: 7.3742\n",
      "Epoch 325/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.0934 - val_loss: 7.0763\n",
      "Epoch 326/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.0355 - val_loss: 6.7151\n",
      "Epoch 327/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.9372 - val_loss: 6.9281\n",
      "Epoch 328/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.9100 - val_loss: 6.7368\n",
      "Epoch 329/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.8770 - val_loss: 6.7996\n",
      "Epoch 330/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.8433 - val_loss: 7.5003\n",
      "Epoch 331/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.3016 - val_loss: 7.0670\n",
      "Epoch 332/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 4.2650 - val_loss: 6.8312\n",
      "Epoch 333/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 4.0220 - val_loss: 7.7528\n",
      "Epoch 334/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4.3630 - val_loss: 6.8215\n",
      "Epoch 335/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.9813 - val_loss: 6.6498\n",
      "Epoch 336/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.7810 - val_loss: 6.8799\n",
      "Epoch 337/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.8473 - val_loss: 6.6497\n",
      "Epoch 338/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.8684 - val_loss: 6.7555\n",
      "Epoch 339/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.9207 - val_loss: 6.7809\n",
      "Epoch 340/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.9619 - val_loss: 7.0419\n",
      "Epoch 341/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.9237 - val_loss: 7.0034\n",
      "Epoch 342/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.8978 - val_loss: 6.7876\n",
      "Epoch 343/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.8272 - val_loss: 6.7168\n",
      "Epoch 344/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.7665 - val_loss: 7.6338\n",
      "Epoch 345/850\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 3.9846 - val_loss: 7.0685\n",
      "Epoch 346/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.9919 - val_loss: 6.7283\n",
      "Epoch 347/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.8627 - val_loss: 6.7535\n",
      "Epoch 348/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.8157 - val_loss: 6.6165\n",
      "Epoch 349/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.7428 - val_loss: 6.5744\n",
      "Epoch 350/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.6988 - val_loss: 6.5875\n",
      "Epoch 351/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.7724 - val_loss: 6.8172\n",
      "Epoch 352/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.9406 - val_loss: 6.7764\n",
      "Epoch 353/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.7878 - val_loss: 6.6579\n",
      "Epoch 354/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.7130 - val_loss: 6.5033\n",
      "Epoch 355/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.8281 - val_loss: 6.6493\n",
      "Epoch 356/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.7394 - val_loss: 6.7838\n",
      "Epoch 357/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.8430 - val_loss: 6.5297\n",
      "Epoch 358/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.7380 - val_loss: 6.7823\n",
      "Epoch 359/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.9024 - val_loss: 7.0989\n",
      "Epoch 360/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.7622 - val_loss: 6.4289\n",
      "Epoch 361/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.6526 - val_loss: 6.5231\n",
      "Epoch 362/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.7392 - val_loss: 6.3634\n",
      "Epoch 363/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.5977 - val_loss: 6.5429\n",
      "Epoch 364/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.7274 - val_loss: 6.4490\n",
      "Epoch 365/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.9048 - val_loss: 6.4747\n",
      "Epoch 366/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.7208 - val_loss: 6.6382\n",
      "Epoch 367/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4.0812 - val_loss: 6.8599\n",
      "Epoch 368/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.9315 - val_loss: 6.3351\n",
      "Epoch 369/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.6267 - val_loss: 6.3038\n",
      "Epoch 370/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.6129 - val_loss: 6.4265\n",
      "Epoch 371/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.7586 - val_loss: 6.3961\n",
      "Epoch 372/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.7116 - val_loss: 6.8359\n",
      "Epoch 373/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.8516 - val_loss: 6.5311\n",
      "Epoch 374/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.8072 - val_loss: 6.8767\n",
      "Epoch 375/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.8389 - val_loss: 7.4736\n",
      "Epoch 376/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.8944 - val_loss: 6.8196\n",
      "Epoch 377/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.9650 - val_loss: 6.6095\n",
      "Epoch 378/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.6241 - val_loss: 6.2698\n",
      "Epoch 379/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.7177 - val_loss: 6.4967\n",
      "Epoch 380/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.6507 - val_loss: 6.3962\n",
      "Epoch 381/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.6612 - val_loss: 6.3490\n",
      "Epoch 382/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.8131 - val_loss: 6.4025\n",
      "Epoch 383/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.6747 - val_loss: 6.4733\n",
      "Epoch 384/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.6840 - val_loss: 6.9828\n",
      "Epoch 385/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.9050 - val_loss: 6.5034\n",
      "Epoch 386/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.8707 - val_loss: 6.3654\n",
      "Epoch 387/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.6972 - val_loss: 6.6042\n",
      "Epoch 388/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.7263 - val_loss: 6.4696\n",
      "Epoch 389/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.5572 - val_loss: 6.3256\n",
      "Epoch 390/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.6356 - val_loss: 6.5867\n",
      "Epoch 391/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.9939 - val_loss: 7.1993\n",
      "Epoch 392/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.7143 - val_loss: 6.7543\n",
      "Epoch 393/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.6851 - val_loss: 6.2925\n",
      "Epoch 394/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.5612 - val_loss: 6.2794\n",
      "Epoch 395/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.6907 - val_loss: 6.5619\n",
      "Epoch 396/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.8814 - val_loss: 6.2358\n",
      "Epoch 397/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.5348 - val_loss: 6.3928\n",
      "Epoch 398/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.6292 - val_loss: 6.2433\n",
      "Epoch 399/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.4557 - val_loss: 6.2264\n",
      "Epoch 400/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.4622 - val_loss: 6.4659\n",
      "Epoch 401/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.5912 - val_loss: 6.3275\n",
      "Epoch 402/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.6380 - val_loss: 6.3504\n",
      "Epoch 403/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.5522 - val_loss: 6.1324\n",
      "Epoch 404/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.4570 - val_loss: 6.5287\n",
      "Epoch 405/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.7951 - val_loss: 6.3209\n",
      "Epoch 406/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.5921 - val_loss: 6.4689\n",
      "Epoch 407/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.6249 - val_loss: 6.5570\n",
      "Epoch 408/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.7082 - val_loss: 6.2120\n",
      "Epoch 409/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.4886 - val_loss: 6.4835\n",
      "Epoch 410/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.6987 - val_loss: 6.3605\n",
      "Epoch 411/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.6563 - val_loss: 6.8263\n",
      "Epoch 412/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.8370 - val_loss: 6.5759\n",
      "Epoch 413/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.6200 - val_loss: 6.2008\n",
      "Epoch 414/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.5266 - val_loss: 6.1975\n",
      "Epoch 415/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.5692 - val_loss: 6.8102\n",
      "Epoch 416/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.4964 - val_loss: 6.4255\n",
      "Epoch 417/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.6658 - val_loss: 6.1896\n",
      "Epoch 418/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.6147 - val_loss: 6.4972\n",
      "Epoch 419/850\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 3.4989 - val_loss: 6.2243\n",
      "Epoch 420/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.5417 - val_loss: 6.2856\n",
      "Epoch 421/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.4465 - val_loss: 6.4356\n",
      "Epoch 422/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.5635 - val_loss: 6.9083\n",
      "Epoch 423/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.6039 - val_loss: 6.2978\n",
      "Epoch 424/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.5310 - val_loss: 6.2400\n",
      "Epoch 425/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.4923 - val_loss: 6.3965\n",
      "Epoch 426/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.5019 - val_loss: 6.3987\n",
      "Epoch 427/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.4611 - val_loss: 6.2656\n",
      "Epoch 428/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.4591 - val_loss: 6.0402\n",
      "Epoch 429/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.4020 - val_loss: 6.2835\n",
      "Epoch 430/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.3783 - val_loss: 6.3027\n",
      "Epoch 431/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.4053 - val_loss: 6.1520\n",
      "Epoch 432/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.3392 - val_loss: 6.1513\n",
      "Epoch 433/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.2788 - val_loss: 6.0546\n",
      "Epoch 434/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.4337 - val_loss: 6.1344\n",
      "Epoch 435/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.3746 - val_loss: 6.0689\n",
      "Epoch 436/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.2974 - val_loss: 6.0610\n",
      "Epoch 437/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.4142 - val_loss: 6.2264\n",
      "Epoch 438/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.4142 - val_loss: 6.1781\n",
      "Epoch 439/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.4121 - val_loss: 6.1700\n",
      "Epoch 440/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.3354 - val_loss: 7.1432\n",
      "Epoch 441/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.6444 - val_loss: 6.6198\n",
      "Epoch 442/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.5878 - val_loss: 6.0449\n",
      "Epoch 443/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.4920 - val_loss: 6.2960\n",
      "Epoch 444/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.3269 - val_loss: 6.0469\n",
      "Epoch 445/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.2921 - val_loss: 6.2674\n",
      "Epoch 446/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.4907 - val_loss: 7.3028\n",
      "Epoch 447/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.7778 - val_loss: 6.2638\n",
      "Epoch 448/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.3351 - val_loss: 6.0985\n",
      "Epoch 449/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.3214 - val_loss: 5.9154\n",
      "Epoch 450/850\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 3.2103 - val_loss: 6.1984\n",
      "Epoch 451/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.4442 - val_loss: 6.9494\n",
      "Epoch 452/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.5261 - val_loss: 6.0099\n",
      "Epoch 453/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.2929 - val_loss: 6.0256\n",
      "Epoch 454/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.3431 - val_loss: 6.2958\n",
      "Epoch 455/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.6575 - val_loss: 6.1701\n",
      "Epoch 456/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.3482 - val_loss: 5.9482\n",
      "Epoch 457/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.2214 - val_loss: 5.9748\n",
      "Epoch 458/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.2194 - val_loss: 5.9958\n",
      "Epoch 459/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.2951 - val_loss: 6.0723\n",
      "Epoch 460/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.2995 - val_loss: 6.0705\n",
      "Epoch 461/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.4240 - val_loss: 6.0089\n",
      "Epoch 462/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.5469 - val_loss: 6.2924\n",
      "Epoch 463/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.3777 - val_loss: 5.9866\n",
      "Epoch 464/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.4262 - val_loss: 6.0600\n",
      "Epoch 465/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.3003 - val_loss: 5.9655\n",
      "Epoch 466/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.2572 - val_loss: 5.9829\n",
      "Epoch 467/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.3176 - val_loss: 6.1654\n",
      "Epoch 468/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.2781 - val_loss: 5.8178\n",
      "Epoch 469/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.2167 - val_loss: 5.9144\n",
      "Epoch 470/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.1828 - val_loss: 5.9100\n",
      "Epoch 471/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.3140 - val_loss: 6.7050\n",
      "Epoch 472/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.6119 - val_loss: 6.2649\n",
      "Epoch 473/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.5182 - val_loss: 6.2275\n",
      "Epoch 474/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.3832 - val_loss: 5.9739\n",
      "Epoch 475/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.5054 - val_loss: 6.5000\n",
      "Epoch 476/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.2841 - val_loss: 5.8868\n",
      "Epoch 477/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.3430 - val_loss: 6.1470\n",
      "Epoch 478/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.4378 - val_loss: 5.9006\n",
      "Epoch 479/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.2752 - val_loss: 6.0958\n",
      "Epoch 480/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.4558 - val_loss: 6.2512\n",
      "Epoch 481/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.3948 - val_loss: 5.7852\n",
      "Epoch 482/850\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3.1825 - val_loss: 5.8911\n",
      "Epoch 483/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.1749 - val_loss: 5.9157\n",
      "Epoch 484/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.2452 - val_loss: 6.0366\n",
      "Epoch 485/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.3225 - val_loss: 6.0864\n",
      "Epoch 486/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 3.4577 - val_loss: 6.9986\n",
      "Epoch 487/850\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 3.4607 - val_loss: 5.9401\n",
      "Epoch 488/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.2066 - val_loss: 5.6895\n",
      "Epoch 489/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.1455 - val_loss: 5.8821\n",
      "Epoch 490/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.1323 - val_loss: 5.7524\n",
      "Epoch 491/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.1661 - val_loss: 5.7433\n",
      "Epoch 492/850\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 3.2081 - val_loss: 5.8658\n",
      "Epoch 493/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 3.1587 - val_loss: 5.8022\n",
      "Epoch 494/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.2294 - val_loss: 6.0003\n",
      "Epoch 495/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.2068 - val_loss: 5.7715\n",
      "Epoch 496/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.1556 - val_loss: 5.7202\n",
      "Epoch 497/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.1169 - val_loss: 5.7285\n",
      "Epoch 498/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.0978 - val_loss: 5.8983\n",
      "Epoch 499/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.1294 - val_loss: 5.8733\n",
      "Epoch 500/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 3.2553 - val_loss: 6.0516\n",
      "Epoch 501/850\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 3.1951 - val_loss: 5.7945\n",
      "Epoch 502/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 3.0882 - val_loss: 5.7659\n",
      "Epoch 503/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.1421 - val_loss: 5.7412\n",
      "Epoch 504/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.1801 - val_loss: 5.7259\n",
      "Epoch 505/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.1156 - val_loss: 6.3973\n",
      "Epoch 506/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.1248 - val_loss: 5.6278\n",
      "Epoch 507/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.0699 - val_loss: 5.5170\n",
      "Epoch 508/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.0738 - val_loss: 5.6218\n",
      "Epoch 509/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 3.0241 - val_loss: 5.7871\n",
      "Epoch 510/850\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 3.0418 - val_loss: 5.8131\n",
      "Epoch 511/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.1939 - val_loss: 5.7183\n",
      "Epoch 512/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.2650 - val_loss: 5.8049\n",
      "Epoch 513/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 3.1668 - val_loss: 5.8563\n",
      "Epoch 514/850\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 3.1914 - val_loss: 5.7831\n",
      "Epoch 515/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.1640 - val_loss: 5.7584\n",
      "Epoch 516/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.0698 - val_loss: 5.8845\n",
      "Epoch 517/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 3.1614 - val_loss: 5.8350\n",
      "Epoch 518/850\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 3.1555 - val_loss: 5.7842\n",
      "Epoch 519/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 3.0347 - val_loss: 5.7863\n",
      "Epoch 520/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.1256 - val_loss: 5.5886\n",
      "Epoch 521/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.1523 - val_loss: 6.0753\n",
      "Epoch 522/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.0908 - val_loss: 5.8314\n",
      "Epoch 523/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.1970 - val_loss: 6.0244\n",
      "Epoch 524/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.2331 - val_loss: 6.1039\n",
      "Epoch 525/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.3876 - val_loss: 6.1342\n",
      "Epoch 526/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.1490 - val_loss: 5.4676\n",
      "Epoch 527/850\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 2.9638 - val_loss: 5.5060\n",
      "Epoch 528/850\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 3.0778 - val_loss: 5.7851\n",
      "Epoch 529/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.0811 - val_loss: 5.5939\n",
      "Epoch 530/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.0743 - val_loss: 5.4902\n",
      "Epoch 531/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.0404 - val_loss: 5.5433\n",
      "Epoch 532/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.1030 - val_loss: 5.8402\n",
      "Epoch 533/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.2410 - val_loss: 5.7013\n",
      "Epoch 534/850\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 3.2340 - val_loss: 5.6984\n",
      "Epoch 535/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.1267 - val_loss: 5.5535\n",
      "Epoch 536/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 3.0784 - val_loss: 5.4478\n",
      "Epoch 537/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 3.1390 - val_loss: 6.5274\n",
      "Epoch 538/850\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 3.3799 - val_loss: 5.9186\n",
      "Epoch 539/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.1783 - val_loss: 5.9409\n",
      "Epoch 540/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.2583 - val_loss: 6.1436\n",
      "Epoch 541/850\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 3.1000 - val_loss: 5.6078\n",
      "Epoch 542/850\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 3.0596 - val_loss: 5.6576\n",
      "Epoch 543/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.0231 - val_loss: 5.7345\n",
      "Epoch 544/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.1217 - val_loss: 5.6267\n",
      "Epoch 545/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.0658 - val_loss: 5.5858\n",
      "Epoch 546/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.1263 - val_loss: 5.6760\n",
      "Epoch 547/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.0291 - val_loss: 5.3852\n",
      "Epoch 548/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.0055 - val_loss: 5.3748\n",
      "Epoch 549/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.9889 - val_loss: 5.4836\n",
      "Epoch 550/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 3.1133 - val_loss: 5.4850\n",
      "Epoch 551/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.1381 - val_loss: 5.8046\n",
      "Epoch 552/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.0315 - val_loss: 5.5207\n",
      "Epoch 553/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.9729 - val_loss: 5.7520\n",
      "Epoch 554/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.0391 - val_loss: 5.4769\n",
      "Epoch 555/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.1818 - val_loss: 5.8352\n",
      "Epoch 556/850\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 3.1167 - val_loss: 5.5385\n",
      "Epoch 557/850\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 2.9660 - val_loss: 5.5122\n",
      "Epoch 558/850\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 3.0398 - val_loss: 5.4880\n",
      "Epoch 559/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.0794 - val_loss: 5.5720\n",
      "Epoch 560/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.0558 - val_loss: 5.5439\n",
      "Epoch 561/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.0555 - val_loss: 5.8434\n",
      "Epoch 562/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.1014 - val_loss: 5.3586\n",
      "Epoch 563/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9344 - val_loss: 5.5866\n",
      "Epoch 564/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9577 - val_loss: 5.3835\n",
      "Epoch 565/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 2.9381 - val_loss: 5.3850\n",
      "Epoch 566/850\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 2.9445 - val_loss: 5.3800\n",
      "Epoch 567/850\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 2.9276 - val_loss: 5.5553\n",
      "Epoch 568/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 3.0489 - val_loss: 5.6641\n",
      "Epoch 569/850\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 2.9625 - val_loss: 5.4613\n",
      "Epoch 570/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 2.9828 - val_loss: 5.3152\n",
      "Epoch 571/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 2.9476 - val_loss: 5.3031\n",
      "Epoch 572/850\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 2.9482 - val_loss: 5.3985\n",
      "Epoch 573/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.9920 - val_loss: 5.2933\n",
      "Epoch 574/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.0073 - val_loss: 5.4450\n",
      "Epoch 575/850\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 3.0190 - val_loss: 5.4131\n",
      "Epoch 576/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9629 - val_loss: 5.3457\n",
      "Epoch 577/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9655 - val_loss: 5.2518\n",
      "Epoch 578/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9240 - val_loss: 5.9819\n",
      "Epoch 579/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9708 - val_loss: 5.3563\n",
      "Epoch 580/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 2.9077 - val_loss: 5.9013\n",
      "Epoch 581/850\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 3.1008 - val_loss: 5.6694\n",
      "Epoch 582/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9566 - val_loss: 5.5476\n",
      "Epoch 583/850\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 2.9150 - val_loss: 5.3421\n",
      "Epoch 584/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9252 - val_loss: 5.3767\n",
      "Epoch 585/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8998 - val_loss: 5.5611\n",
      "Epoch 586/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.9988 - val_loss: 5.4393\n",
      "Epoch 587/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.0166 - val_loss: 5.4526\n",
      "Epoch 588/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.0714 - val_loss: 5.4910\n",
      "Epoch 589/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 2.9736 - val_loss: 5.4685\n",
      "Epoch 590/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.9870 - val_loss: 5.4410\n",
      "Epoch 591/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.0905 - val_loss: 5.4308\n",
      "Epoch 592/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.0314 - val_loss: 5.5430\n",
      "Epoch 593/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9289 - val_loss: 5.4078\n",
      "Epoch 594/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9487 - val_loss: 5.5451\n",
      "Epoch 595/850\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 2.9575 - val_loss: 5.3778\n",
      "Epoch 596/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.0157 - val_loss: 5.5503\n",
      "Epoch 597/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.9561 - val_loss: 5.6135\n",
      "Epoch 598/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9814 - val_loss: 5.7177\n",
      "Epoch 599/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 3.0195 - val_loss: 5.2655\n",
      "Epoch 600/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9238 - val_loss: 5.5147\n",
      "Epoch 601/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.9056 - val_loss: 5.3031\n",
      "Epoch 602/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.0055 - val_loss: 5.4094\n",
      "Epoch 603/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9806 - val_loss: 5.3012\n",
      "Epoch 604/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.9907 - val_loss: 5.2788\n",
      "Epoch 605/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.0989 - val_loss: 5.3575\n",
      "Epoch 606/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 3.0452 - val_loss: 5.4559\n",
      "Epoch 607/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3.0049 - val_loss: 5.3286\n",
      "Epoch 608/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 2.9392 - val_loss: 5.2063\n",
      "Epoch 609/850\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 2.8897 - val_loss: 5.1778\n",
      "Epoch 610/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9202 - val_loss: 5.1597\n",
      "Epoch 611/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.9539 - val_loss: 5.2536\n",
      "Epoch 612/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9315 - val_loss: 5.4557\n",
      "Epoch 613/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 3.1019 - val_loss: 5.3564\n",
      "Epoch 614/850\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 2.9132 - val_loss: 5.9078\n",
      "Epoch 615/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9825 - val_loss: 5.5355\n",
      "Epoch 616/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 3.1121 - val_loss: 5.3817\n",
      "Epoch 617/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8753 - val_loss: 5.1476\n",
      "Epoch 618/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9992 - val_loss: 5.3313\n",
      "Epoch 619/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 2.8853 - val_loss: 5.4320\n",
      "Epoch 620/850\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 2.9497 - val_loss: 5.3710\n",
      "Epoch 621/850\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 3.0567 - val_loss: 5.6358\n",
      "Epoch 622/850\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 3.0737 - val_loss: 5.1298\n",
      "Epoch 623/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.1181 - val_loss: 5.6354\n",
      "Epoch 624/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9984 - val_loss: 5.1090\n",
      "Epoch 625/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.8166 - val_loss: 5.1451\n",
      "Epoch 626/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.8241 - val_loss: 5.2449\n",
      "Epoch 627/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 2.8970 - val_loss: 5.2236\n",
      "Epoch 628/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.8822 - val_loss: 5.3625\n",
      "Epoch 629/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8810 - val_loss: 5.2268\n",
      "Epoch 630/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8366 - val_loss: 5.3023\n",
      "Epoch 631/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8160 - val_loss: 5.0963\n",
      "Epoch 632/850\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 2.8797 - val_loss: 5.3063\n",
      "Epoch 633/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8996 - val_loss: 5.3338\n",
      "Epoch 634/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 2.9492 - val_loss: 5.4451\n",
      "Epoch 635/850\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 2.9940 - val_loss: 5.2921\n",
      "Epoch 636/850\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 2.7943 - val_loss: 5.3103\n",
      "Epoch 637/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.7942 - val_loss: 5.5912\n",
      "Epoch 638/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.8723 - val_loss: 5.5013\n",
      "Epoch 639/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.8030 - val_loss: 5.2429\n",
      "Epoch 640/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8176 - val_loss: 5.2874\n",
      "Epoch 641/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8032 - val_loss: 5.2838\n",
      "Epoch 642/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 2.8027 - val_loss: 5.2785\n",
      "Epoch 643/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.7829 - val_loss: 5.1355\n",
      "Epoch 644/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8365 - val_loss: 5.2764\n",
      "Epoch 645/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9133 - val_loss: 5.3895\n",
      "Epoch 646/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8325 - val_loss: 5.5356\n",
      "Epoch 647/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8828 - val_loss: 6.2542\n",
      "Epoch 648/850\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 3.3338 - val_loss: 5.6593\n",
      "Epoch 649/850\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 3.1924 - val_loss: 5.4769\n",
      "Epoch 650/850\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 2.9507 - val_loss: 5.8475\n",
      "Epoch 651/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.8977 - val_loss: 5.3970\n",
      "Epoch 652/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8614 - val_loss: 5.2947\n",
      "Epoch 653/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8963 - val_loss: 5.0867\n",
      "Epoch 654/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.7979 - val_loss: 5.1875\n",
      "Epoch 655/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8094 - val_loss: 5.1838\n",
      "Epoch 656/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8224 - val_loss: 5.1616\n",
      "Epoch 657/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9844 - val_loss: 5.3659\n",
      "Epoch 658/850\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 2.9025 - val_loss: 5.3329\n",
      "Epoch 659/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.9044 - val_loss: 5.2635\n",
      "Epoch 660/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.9018 - val_loss: 5.3706\n",
      "Epoch 661/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.8617 - val_loss: 5.0827\n",
      "Epoch 662/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.7869 - val_loss: 5.3261\n",
      "Epoch 663/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.9238 - val_loss: 5.5546\n",
      "Epoch 664/850\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 3.0139 - val_loss: 5.6753\n",
      "Epoch 665/850\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2.9962 - val_loss: 5.2734\n",
      "Epoch 666/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 2.9363 - val_loss: 5.2946\n",
      "Epoch 667/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.8790 - val_loss: 5.3078\n",
      "Epoch 668/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.8228 - val_loss: 5.3003\n",
      "Epoch 669/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.8498 - val_loss: 5.2410\n",
      "Epoch 670/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.7911 - val_loss: 5.2007\n",
      "Epoch 671/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.7863 - val_loss: 5.1233\n",
      "Epoch 672/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.7477 - val_loss: 5.1233\n",
      "Epoch 673/850\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 2.7738 - val_loss: 5.0905\n",
      "Epoch 674/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.7760 - val_loss: 4.9588\n",
      "Epoch 675/850\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 2.7912 - val_loss: 5.4573\n",
      "Epoch 676/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9358 - val_loss: 5.3650\n",
      "Epoch 677/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.8230 - val_loss: 5.0874\n",
      "Epoch 678/850\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 2.7778 - val_loss: 5.2507\n",
      "Epoch 679/850\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 2.8628 - val_loss: 5.2189\n",
      "Epoch 680/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8093 - val_loss: 5.4246\n",
      "Epoch 681/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.8179 - val_loss: 5.1911\n",
      "Epoch 682/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.7618 - val_loss: 5.0742\n",
      "Epoch 683/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8261 - val_loss: 5.2288\n",
      "Epoch 684/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8321 - val_loss: 5.2298\n",
      "Epoch 685/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9401 - val_loss: 5.5687\n",
      "Epoch 686/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.0094 - val_loss: 5.3207\n",
      "Epoch 687/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3.1174 - val_loss: 5.2004\n",
      "Epoch 688/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.8399 - val_loss: 5.2209\n",
      "Epoch 689/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.7936 - val_loss: 5.1031\n",
      "Epoch 690/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.7377 - val_loss: 5.0550\n",
      "Epoch 691/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.7119 - val_loss: 5.1748\n",
      "Epoch 692/850\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 2.7683 - val_loss: 5.2522\n",
      "Epoch 693/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.7786 - val_loss: 5.1271\n",
      "Epoch 694/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.7390 - val_loss: 5.1508\n",
      "Epoch 695/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.7358 - val_loss: 5.4853\n",
      "Epoch 696/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.8074 - val_loss: 4.9723\n",
      "Epoch 697/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.7925 - val_loss: 5.0716\n",
      "Epoch 698/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8247 - val_loss: 5.1304\n",
      "Epoch 699/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8945 - val_loss: 5.1440\n",
      "Epoch 700/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 2.8521 - val_loss: 5.0951\n",
      "Epoch 701/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.8993 - val_loss: 5.4605\n",
      "Epoch 702/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 2.9158 - val_loss: 5.7120\n",
      "Epoch 703/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9029 - val_loss: 5.0493\n",
      "Epoch 704/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.7263 - val_loss: 4.9984\n",
      "Epoch 705/850\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 2.7548 - val_loss: 5.1331\n",
      "Epoch 706/850\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 2.7224 - val_loss: 5.0039\n",
      "Epoch 707/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.7125 - val_loss: 5.3162\n",
      "Epoch 708/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.7502 - val_loss: 5.0993\n",
      "Epoch 709/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.7750 - val_loss: 5.0414\n",
      "Epoch 710/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.7646 - val_loss: 5.0547\n",
      "Epoch 711/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.7395 - val_loss: 4.9955\n",
      "Epoch 712/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.7106 - val_loss: 5.3806\n",
      "Epoch 713/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.7770 - val_loss: 5.0613\n",
      "Epoch 714/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9164 - val_loss: 5.4006\n",
      "Epoch 715/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.9416 - val_loss: 5.2529\n",
      "Epoch 716/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8148 - val_loss: 5.4248\n",
      "Epoch 717/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8145 - val_loss: 5.2101\n",
      "Epoch 718/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.8090 - val_loss: 5.3182\n",
      "Epoch 719/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.7363 - val_loss: 5.1130\n",
      "Epoch 720/850\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 2.6807 - val_loss: 5.1248\n",
      "Epoch 721/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.6470 - val_loss: 5.2328\n",
      "Epoch 722/850\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2.7155 - val_loss: 5.1678\n",
      "Epoch 723/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.7897 - val_loss: 5.1705\n",
      "Epoch 724/850\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.6770 - val_loss: 5.1551\n",
      "CPU times: total: 16 s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=50)\n",
    "\n",
    "history = dnn_model.fit(\n",
    "    scaled_train_features,\n",
    "    train_labels,\n",
    "    validation_split=0.2,\n",
    "    verbose=1, \n",
    "    epochs=best_parameters['epochs'],\n",
    "    batch_size=best_parameters['batch_size'],\n",
    "    use_multiprocessing=True,\n",
    "    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.958810329437256"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(history, num_epochs=10)\n",
    "\n",
    "plt.savefig('plot_loss_original.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-15.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "test_predictions = dnn_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.41953"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-15., -15.,   0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(851, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_predictions = dnn_model.predict(test_features)#.flatten()\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(test_labels, test_predictions)\n",
    "plt.xlabel('True Values [MPG]')\n",
    "plt.ylabel('Predictions [MPG]')\n",
    "lims = [0, 50]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "fig = plt.plot(lims, lims)\n",
    "\n",
    "plt.savefig('test_predictions_original.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
